{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Loader_17 import DAVIS_Rawset, DAVIS_Infer, DAVIS_Dataset, normalize\n",
    "from polygon import RasLoss, SoftPolygon\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from MyLoss import deviation_loss, total_len_loss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from ms_deform_attn import MSDeformAttn\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import random\n",
    "import gc\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rawset = DAVIS_Rawset(is_train=True)\n",
    "val_rawset = DAVIS_Rawset(is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")\n",
    "\n",
    "\n",
    "def get_valid_ratio(mask):\n",
    "    _, H, W = mask.shape\n",
    "    valid_H = torch.sum(~mask[:, :, 0], 1)\n",
    "    valid_W = torch.sum(~mask[:, 0, :], 1)\n",
    "    valid_ratio_h = valid_H.float() / H\n",
    "    valid_ratio_w = valid_W.float() / W\n",
    "    valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n",
    "    return valid_ratio\n",
    "\n",
    "\n",
    "class DeformableTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        d_ffn=1024,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        n_levels=4,\n",
    "        n_heads=8,\n",
    "        n_points=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, src):\n",
    "        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout3(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        pos,\n",
    "        reference_points,\n",
    "        spatial_shapes,\n",
    "        level_start_index,\n",
    "        padding_mask=None,\n",
    "    ):\n",
    "        # self attention\n",
    "        src2 = self.self_attn(\n",
    "            self.with_pos_embed(src, pos),\n",
    "            reference_points,\n",
    "            src,\n",
    "            spatial_shapes,\n",
    "            level_start_index,\n",
    "            padding_mask,\n",
    "        )\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # ffn\n",
    "        src = self.forward_ffn(src)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class DeformableTransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reference_points(spatial_shapes, valid_ratios, device):\n",
    "        reference_points_list = []\n",
    "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
    "\n",
    "            ref_y, ref_x = torch.meshgrid(\n",
    "                torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n",
    "                torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device),\n",
    "            )\n",
    "            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n",
    "            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)\n",
    "            ref = torch.stack((ref_x, ref_y), -1)\n",
    "            reference_points_list.append(ref)\n",
    "        reference_points = torch.cat(reference_points_list, 1)\n",
    "        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n",
    "        return reference_points\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        spatial_shapes,\n",
    "        level_start_index,\n",
    "        valid_ratios,\n",
    "        pos=None,\n",
    "        padding_mask=None,\n",
    "    ):\n",
    "        output = src\n",
    "        reference_points = self.get_reference_points(\n",
    "            spatial_shapes, valid_ratios, device=src.device\n",
    "        )\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            output = layer(\n",
    "                output,\n",
    "                pos,\n",
    "                reference_points,\n",
    "                spatial_shapes,\n",
    "                level_start_index,\n",
    "                padding_mask,\n",
    "            )\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_bou_feats(img_feats: torch.Tensor, boundary: torch.Tensor) -> torch.Tensor:\n",
    "    return img_feats[\n",
    "        torch.arange(boundary.shape[0]).unsqueeze(1),\n",
    "        :,\n",
    "        boundary[:, :, 1],\n",
    "        boundary[:, :, 0],\n",
    "    ]\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout=0.1, max_seq_len=102400) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, embed_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def get_img_tokens(img_features: torch.Tensor) -> torch.Tensor:\n",
    "    img_tokens = rearrange(\n",
    "        img_features,\n",
    "        \"b c h w -> b (h w) c\",\n",
    "    )\n",
    "    return img_tokens\n",
    "\n",
    "\n",
    "def get_extrame_4_points(batch_indices: torch.Tensor):\n",
    "    if batch_indices.shape[0] == 0:\n",
    "        mid = 224 // 2\n",
    "        return torch.tensor([[mid, mid], [mid, mid], [mid, mid], [mid, mid]])\n",
    "    x_min = batch_indices[:, 1].min()\n",
    "    x_max = batch_indices[:, 1].max()\n",
    "    y_min = batch_indices[:, 0].min()\n",
    "    y_max = batch_indices[:, 0].max()\n",
    "    return torch.tensor(\n",
    "        [[x_min, y_min], [x_min, y_max], [x_max, y_max], [x_max, y_min]]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_bounding_box(sgm: torch.Tensor):\n",
    "    indices = torch.nonzero(sgm)\n",
    "    batch_size = sgm.shape[0]\n",
    "    bounding_boxes = []\n",
    "    for i in range(batch_size):\n",
    "        batch_indices = indices[indices[:, 0] == i][:, 1:]\n",
    "        bounding_boxes.append(get_extrame_4_points(batch_indices))\n",
    "    return torch.stack(bounding_boxes)\n",
    "\n",
    "\n",
    "def add_mid_points(points: torch.Tensor) -> torch.Tensor:\n",
    "    points_shift = torch.roll(points, 1, 1)\n",
    "    mid_points = (points + points_shift) / 2\n",
    "    new_points = torch.zeros((points.shape[0], points.shape[1] * 2, 2)).to(\n",
    "        points.device\n",
    "    )\n",
    "    new_points[:, ::2] = mid_points\n",
    "    new_points[:, 1::2] = points\n",
    "    return new_points\n",
    "\n",
    "\n",
    "class DeformableTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        d_ffn=1024,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        n_levels=4,\n",
    "        n_heads=8,\n",
    "        n_points=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # cross attention\n",
    "        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        query_pos,\n",
    "        reference_points,\n",
    "        src,\n",
    "        src_spatial_shapes,\n",
    "        level_start_index,\n",
    "        src_padding_mask=None,\n",
    "    ):\n",
    "        # self attention\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(\n",
    "            q.transpose(0, 1), k.transpose(0, 1), tgt.transpose(0, 1)\n",
    "        )[0].transpose(0, 1)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # cross attention\n",
    "        tgt2 = self.cross_attn(\n",
    "            self.with_pos_embed(tgt, query_pos),\n",
    "            reference_points,\n",
    "            src,\n",
    "            src_spatial_shapes,\n",
    "            level_start_index,\n",
    "            src_padding_mask,\n",
    "        )\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # ffn\n",
    "        tgt = self.forward_ffn(tgt)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class DeformableTransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        reference_points,\n",
    "        src,\n",
    "        src_spatial_shapes,\n",
    "        src_level_start_index,\n",
    "        src_valid_ratios,\n",
    "        query_pos=None,\n",
    "        src_padding_mask=None,\n",
    "    ):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "        intermediate_reference_points = []\n",
    "        for lid, layer in enumerate(self.layers):\n",
    "            if reference_points.shape[-1] == 4:\n",
    "                reference_points_input = (\n",
    "                    reference_points[:, :, None]\n",
    "                    * torch.cat([src_valid_ratios, src_valid_ratios], -1)[:, None]\n",
    "                )\n",
    "            else:\n",
    "                assert reference_points.shape[-1] == 2\n",
    "                reference_points_input = (\n",
    "                    reference_points[:, :, None] * src_valid_ratios[:, None]\n",
    "                )\n",
    "            output = layer(\n",
    "                output,\n",
    "                query_pos,\n",
    "                reference_points_input,\n",
    "                src,\n",
    "                src_spatial_shapes,\n",
    "                src_level_start_index,\n",
    "                src_padding_mask,\n",
    "            )\n",
    "\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "                intermediate_reference_points.append(reference_points)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n",
    "\n",
    "        return output, reference_points\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def inverse_sigmoid(x, eps=1e-5):\n",
    "    x = x.clamp(min=0, max=1)\n",
    "    x1 = x.clamp(min=eps)\n",
    "    x2 = (1 - x).clamp(min=eps)\n",
    "    return torch.log(x1 / x2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_bou_iou(\n",
    "    index: int,\n",
    "    boundary: torch.Tensor,\n",
    "    mask: torch.Tensor,\n",
    "    rasterizer,\n",
    ") -> torch.Tensor:\n",
    "    pred_sgm = rasterizer(boundary, 224, 224)\n",
    "    pred_sgm[pred_sgm == -1] = 0\n",
    "    pred_sgm = pred_sgm[index]\n",
    "    boundary = boundary[index]\n",
    "    mask = mask[index]\n",
    "    intersection = pred_sgm * mask\n",
    "    intersection = intersection.sum()\n",
    "    union = pred_sgm.sum() + mask.sum() - intersection.sum()\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "def get_batch_average_bou_iou(\n",
    "    boundary: torch.Tensor,\n",
    "    mask: torch.Tensor,\n",
    "    rasterizer,\n",
    ") -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        pred_sgm = rasterizer(boundary, 224, 224)\n",
    "        pred_sgm[pred_sgm == -1] = 0\n",
    "        pred_sgm = pred_sgm.flatten(1)\n",
    "        mask = mask.flatten(1)\n",
    "        intersection = pred_sgm * mask\n",
    "        intersection = intersection.sum(-1)\n",
    "        union = pred_sgm.sum(-1) + mask.sum(-1) - intersection\n",
    "        iou = intersection / union\n",
    "        return iou.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformLearnImage(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_num=1,\n",
    "        up_scale_num=4,\n",
    "        head_num=6,\n",
    "        medium_level_size=[14, 28, 56, 112],\n",
    "        offset_limit=56,\n",
    "        n_points=4,\n",
    "        freeze_backbone=True,\n",
    "    ) -> None:\n",
    "        super(DeformLearnImage, self).__init__()\n",
    "        self.up_scale_num = up_scale_num\n",
    "        self.offset_limit = offset_limit\n",
    "        self.medium_level_size = medium_level_size\n",
    "        self.featup = torch.hub.load(\n",
    "            \"mhamilton723/FeatUp\",\n",
    "            \"dino16\",\n",
    "            use_norm=True,\n",
    "        ).cuda()\n",
    "        if freeze_backbone:\n",
    "            for param in self.featup.parameters():\n",
    "                param.requires_grad = False\n",
    "        d_model = 384\n",
    "        d_ffn = 1024\n",
    "        n_levels = len(medium_level_size) + 1\n",
    "        self.pos_enoc = PositionalEncoding(d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        deform_encoder_layer = DeformableTransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            d_ffn=d_ffn,\n",
    "            n_levels=n_levels,\n",
    "            n_heads=head_num,\n",
    "            n_points=n_points,\n",
    "        )\n",
    "        self.deform_encoder = DeformableTransformerEncoder(\n",
    "            deform_encoder_layer,\n",
    "            num_layers=layer_num,\n",
    "        )\n",
    "        query_num = 4\n",
    "        for i in range(up_scale_num):\n",
    "            query_num *= 2\n",
    "        self.query_embed = nn.Embedding(query_num, d_model)\n",
    "        # init the query embedding\n",
    "        xavier_uniform_(self.query_embed.weight)\n",
    "        deform_decoder_layer = DeformableTransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            d_ffn=d_ffn,\n",
    "            n_levels=n_levels,\n",
    "            n_heads=head_num,\n",
    "            n_points=n_points,\n",
    "        )\n",
    "        deform_decoder = DeformableTransformerDecoder(\n",
    "            deform_decoder_layer,\n",
    "            num_layers=layer_num,\n",
    "        )\n",
    "        self.deform_decoders = _get_clones(deform_decoder, up_scale_num + 1)\n",
    "        xy_fc = MLP(d_model, d_model, 2, 3).cuda()\n",
    "        # xy_fc = nn.Linear(d_model, 2).cuda()\n",
    "        self.xy_fc = _get_clones(xy_fc, up_scale_num + 1)\n",
    "\n",
    "    def get_valid_ratio(self, mask: torch.Tensor):\n",
    "        _, H, W = mask.shape\n",
    "        valid_H = torch.sum(~mask[:, :, 0], 1)\n",
    "        valid_W = torch.sum(~mask[:, 0, :], 1)\n",
    "        valid_ratio_h = valid_H.float() / H\n",
    "        valid_ratio_w = valid_W.float() / W\n",
    "        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n",
    "        return valid_ratio\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        img: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ):\n",
    "        feats = self.featup(img)\n",
    "        # prepare the input for the MSDeformAttn module\n",
    "        srcs = []\n",
    "        padding_masks = []\n",
    "        for low_res in self.medium_level_size:\n",
    "            srcs.append(\n",
    "                F.interpolate(\n",
    "                    feats,\n",
    "                    size=(low_res, low_res),\n",
    "                    mode=\"bilinear\",\n",
    "                ),\n",
    "            )\n",
    "        srcs.append(feats)\n",
    "        for src in srcs:\n",
    "            padding_masks.append(torch.zeros_like(src[:, 0:1, :, :]).squeeze(1).bool())\n",
    "        src_flatten = []\n",
    "        spatial_shapes = []\n",
    "        for src in srcs:\n",
    "            src_flatten.append(\n",
    "                rearrange(src, \"b c h w -> b (h w) c\"),\n",
    "            )\n",
    "            spatial_shapes.append(src.shape[-2:])\n",
    "        level_start_index = torch.cat(\n",
    "            (\n",
    "                torch.tensor([0]),\n",
    "                torch.cumsum(\n",
    "                    torch.tensor([x.shape[1] for x in src_flatten]),\n",
    "                    0,\n",
    "                )[:-1],\n",
    "            )\n",
    "        ).cuda()\n",
    "        src_flatten = torch.cat(src_flatten, 1).cuda()\n",
    "        valid_ratios = torch.stack(\n",
    "            [self.get_valid_ratio(mask) for mask in padding_masks],\n",
    "            1,\n",
    "        ).cuda()\n",
    "        spatial_shapes = torch.as_tensor(\n",
    "            spatial_shapes,\n",
    "            dtype=torch.long,\n",
    "            device=src_flatten.device,\n",
    "        )\n",
    "        src_flatten = self.layer_norm(src_flatten)\n",
    "        src_flatten = self.pos_enoc(src_flatten)\n",
    "        src_flatten = self.deform_encoder(\n",
    "            src=src_flatten,\n",
    "            spatial_shapes=spatial_shapes,\n",
    "            level_start_index=level_start_index,\n",
    "            valid_ratios=valid_ratios,\n",
    "        )\n",
    "\n",
    "        B, S, C = src_flatten.shape\n",
    "        queries = self.query_embed.weight.unsqueeze(0).repeat(B, 1, 1).cuda()\n",
    "        init_bou = get_bounding_box(mask).cuda() / 224\n",
    "        current_query_num = init_bou.shape[1]\n",
    "        current_query = queries[:, :current_query_num]\n",
    "        decode_output, _ = self.deform_decoders[0](\n",
    "            current_query,\n",
    "            init_bou,\n",
    "            src_flatten,\n",
    "            spatial_shapes,\n",
    "            level_start_index,\n",
    "            valid_ratios,\n",
    "        )\n",
    "\n",
    "        xy_offset = (\n",
    "            (self.xy_fc[0](decode_output).sigmoid() - 0.5) * self.offset_limit / 224\n",
    "        )\n",
    "        init_bou += xy_offset\n",
    "\n",
    "        # xy_offset = self.xy_fc[0](decode_output)\n",
    "        # xy_offset = xy_offset.sigmoid()\n",
    "        # init_bou = inverse_sigmoid(init_bou) + xy_offset\n",
    "        # init_bou = init_bou.sigmoid().clone()\n",
    "\n",
    "        # init_bou = init_bou.clamp(0, 1)\n",
    "\n",
    "        results = [init_bou]\n",
    "        for i in range(self.up_scale_num):\n",
    "            new_query = queries[:, current_query_num : current_query_num * 2]\n",
    "            current_query_num *= 2\n",
    "\n",
    "            current_query = torch.zeros((B, current_query_num, C)).to(\n",
    "                src_flatten.device\n",
    "            )\n",
    "            current_query[:, ::2] = new_query\n",
    "            current_query[:, 1::2] = decode_output\n",
    "            # current_query = torch.cat([new_query, decode_output], 1)\n",
    "\n",
    "            cur_bou = add_mid_points(results[-1])\n",
    "            decode_output, _ = self.deform_decoders[i + 1](\n",
    "                current_query,\n",
    "                cur_bou,\n",
    "                src_flatten,\n",
    "                spatial_shapes,\n",
    "                level_start_index,\n",
    "                valid_ratios,\n",
    "            )\n",
    "\n",
    "            xy_offset = (\n",
    "                (self.xy_fc[i + 1](decode_output).sigmoid() - 0.5)\n",
    "                * self.offset_limit\n",
    "                / 224\n",
    "            )\n",
    "            cur_bou += xy_offset\n",
    "\n",
    "            # xy_offset = self.xy_fc[i + 1](decode_output)\n",
    "            # cur_bou = inverse_sigmoid(cur_bou) + xy_offset\n",
    "            # cur_bou = cur_bou.sigmoid().clone()\n",
    "\n",
    "            # cur_bou = cur_bou.clamp(0, 1)\n",
    "\n",
    "            results.append(cur_bou)\n",
    "        results = [result * 224 for result in results]\n",
    "\n",
    "        if self.training:\n",
    "            return results\n",
    "        else:\n",
    "            result = results[-1]\n",
    "            result = result.clamp(0, 223)\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeformLearnImage().cuda()\n",
    "model.train()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAVIS_IMG_Dataset(Dataset):\n",
    "    def __init__(self, rawset: DAVIS_Rawset, is_train: bool, val_sample_num: int = 4):\n",
    "        self.is_train = is_train\n",
    "        if not is_train:\n",
    "            self.val_sample_num = val_sample_num\n",
    "        # remove the empty frame\n",
    "        empty_frame_idx = []\n",
    "        for video_idx, video_data in enumerate(rawset.data_set):\n",
    "            for frame_idx, frame_data in enumerate(video_data):\n",
    "                img, mask = frame_data\n",
    "                if mask.sum() == 0:\n",
    "                    empty_frame_idx.append((video_idx, frame_idx))\n",
    "        self.data_set = []\n",
    "        # add the data without empty frame\n",
    "        for video_idx, video_data in enumerate(rawset.data_set):\n",
    "            self.data_set.append([])\n",
    "            for frame_idx, frame_data in enumerate(video_data):\n",
    "                if (video_idx, frame_idx) in empty_frame_idx:\n",
    "                    continue\n",
    "                img, mask = frame_data\n",
    "                self.data_set[-1].append((img, mask))\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.data_set)\n",
    "        else:\n",
    "            return len(self.data_set) * self.val_sample_num\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if self.is_train:\n",
    "            video_idx = idx\n",
    "            video_data = self.data_set[video_idx]\n",
    "            # random select one frame\n",
    "            frame_idx = random.randint(0, len(video_data) - 1)\n",
    "            img, mask = video_data[frame_idx]\n",
    "            return img, mask\n",
    "        else:\n",
    "            # get the video index and frame index\n",
    "            video_idx = idx // self.val_sample_num\n",
    "            video_data = self.data_set[video_idx]\n",
    "            video_data_len = len(video_data)\n",
    "            frame_step = video_data_len // self.val_sample_num\n",
    "            frame_idx = (idx % self.val_sample_num) * frame_step\n",
    "            img, mask = video_data[frame_idx]\n",
    "            return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train_dataset = DAVIS_IMG_Dataset(train_rawset, is_train=True)\n",
    "img_val_dataset = DAVIS_IMG_Dataset(val_rawset, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_train_dataset), len(img_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 9\n",
    "first_frame, mask = img_val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_train_dataset), len(img_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train dataset\n",
    "train_loader = DataLoader(img_train_dataset, batch_size=1, shuffle=True)\n",
    "first_frame, mask = next(iter(train_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(normalize(first_frame[0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test val dataset\n",
    "val_loader = DataLoader(img_val_dataset, batch_size=2, shuffle=True)\n",
    "first_frame, mask = next(iter(val_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(normalize(first_frame[0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ras_loss = RasLoss().cuda()\n",
    "gt_rasterizer = SoftPolygon(1, \"hard_mask\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeformLearnImage().cuda()\n",
    "model.train()\n",
    "results = model(first_frame.cuda(), mask.cuda())\n",
    "results[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(img_train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(img_val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeformLearnImage().cuda()\n",
    "loss_dict = {}\n",
    "iou_train_dict = {}\n",
    "iou_val_dict = {}\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epoch_num = 20\n",
    "eval_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epoch_num):\n",
    "    mean_loss = 0\n",
    "    train_mean_iou = 0\n",
    "    for first_frame, mask in tqdm(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        results = model(first_frame.cuda(), mask.cuda())\n",
    "        loss = 0\n",
    "        for result in results:\n",
    "            loss += ras_loss(result, mask.cuda())\n",
    "        loss /= len(results)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_loss += loss.item()\n",
    "        iou = get_batch_average_bou_iou(results[-1], mask.cuda(), gt_rasterizer)\n",
    "        train_mean_iou += iou.item()\n",
    "    mean_loss /= len(train_loader)\n",
    "    train_mean_iou /= len(train_loader)\n",
    "    loss_dict[e] = mean_loss\n",
    "    iou_train_dict[e] = train_mean_iou\n",
    "    print(f\"Epoch {e} train loss: {mean_loss:.4f}, iou: {train_mean_iou:.4f}\")\n",
    "    if e % eval_period == 0 or e == epoch_num - 1:\n",
    "        val_mean_iou = 0\n",
    "        model.eval()\n",
    "        for first_frame, mask in tqdm(val_loader):\n",
    "            result = model(first_frame.cuda(), mask.cuda())\n",
    "            iou = get_batch_average_bou_iou(result, mask.cuda(), gt_rasterizer)\n",
    "            val_mean_iou += iou.item()\n",
    "        val_mean_iou /= len(val_loader)\n",
    "        iou_val_dict[e] = val_mean_iou\n",
    "        print(f\"Epoch {e} val iou: {val_mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deform_img_davis\"\n",
    "model_path = f\"./model/{model_name}_best.pth\"\n",
    "log_dir = f\"./log/{model_name}\"\n",
    "# load the log\n",
    "with open(f\"{log_dir}/loss.json\", \"r\") as f:\n",
    "    loss_dict = json.load(f)\n",
    "with open(f\"{log_dir}/iou_train.json\", \"r\") as f:\n",
    "    iou_train_dict = json.load(f)\n",
    "with open(f\"{log_dir}/iou_val.json\", \"r\") as f:\n",
    "    iou_val_dict = json.load(f)\n",
    "# plot the loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_dict.keys(), loss_dict.values(), label=\"train loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.xticks(np.arange(0, 1500, 100))\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"train loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plot the iou\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iou_train_dict.keys(), iou_train_dict.values(), label=\"train iou\")\n",
    "plt.plot(iou_val_dict.keys(), iou_val_dict.values(), label=\"val iou\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.xticks(np.arange(0, 1500, 100))\n",
    "plt.ylabel(\"iou\")\n",
    "plt.title(\"iou\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# print the best iou in train and val\n",
    "best_train_iou = max(iou_train_dict.values())\n",
    "best_val_iou = max(iou_val_dict.values())\n",
    "print(f\"best train iou: {best_train_iou:.4f}, best val iou: {best_val_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeformLearnImage().cuda()\n",
    "# load the best model\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "first_frame, mask = next(iter(val_loader))\n",
    "index = 0\n",
    "model.eval()\n",
    "pred_bou = model(first_frame.cuda(), mask.cuda())\n",
    "# calculate the iou\n",
    "iou = get_bou_iou(index, pred_bou, mask.cuda(), gt_rasterizer)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f\"iou: {iou:.4f}\")\n",
    "plt.imshow(normalize(first_frame[0]).permute(1, 2, 0))\n",
    "pred_bou_np = pred_bou[index].detach().cpu().numpy()\n",
    "plt.plot(pred_bou_np[:, 0], pred_bou_np[:, 1], \"r\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask[index])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text_idxs = np.arange(0, len(img_val_dataset), 4)\n",
    "# get the text data\n",
    "val_text_data = []\n",
    "for idx in val_text_idxs:\n",
    "    first_frame, mask = img_val_dataset[idx]\n",
    "    val_text_data.append((first_frame, mask))\n",
    "# show the text data\n",
    "plt.figure(figsize=(10, 5 * len(val_text_data)))\n",
    "for i, (first_frame, mask) in enumerate(val_text_data):\n",
    "    plt.subplot(len(val_text_data), 2, 2 * i + 1)\n",
    "    plt.imshow(normalize(first_frame).permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    model.eval()\n",
    "    pred_bou = model(first_frame.unsqueeze(0).cuda(), mask.unsqueeze(0).cuda())\n",
    "    iou = get_bou_iou(0, pred_bou, mask.unsqueeze(0).cuda(), gt_rasterizer)\n",
    "    plt.title(f\"iou: {iou:.4f}\")\n",
    "    pred_bou_np = pred_bou[0].detach().cpu().numpy()\n",
    "    plt.plot(pred_bou_np[:, 0], pred_bou_np[:, 1], \"r\")\n",
    "    plt.scatter(pred_bou_np[:, 0], pred_bou_np[:, 1], c=\"r\", s=5)\n",
    "    plt.subplot(len(val_text_data), 2, 2 * i + 2)\n",
    "    plt.imshow(mask)\n",
    "    plt.title(f\"ground truth, {i}\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we start the video part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_point_path = \"sample_results/train_256_uniform.json\"\n",
    "val_point_path = \"sample_results/val_256_uniform.json\"\n",
    "with open(train_point_path, \"r\") as f:\n",
    "    train_points = json.load(f)\n",
    "with open(val_point_path, \"r\") as f:\n",
    "    val_points = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(train_points[100][0][\"4\"][\"boundary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAVIS_withPoint(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        raw_set: DAVIS_Rawset,\n",
    "        point_num: int,\n",
    "        is_train: bool,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.point_num = point_num\n",
    "        # remove all the video with empty frame\n",
    "        empty_video_idx = []\n",
    "        for video_idx, video_data in enumerate(raw_set.data_set):\n",
    "            for frame_data in video_data:\n",
    "                img, mask = frame_data\n",
    "                if mask.sum() == 0:\n",
    "                    empty_video_idx.append(video_idx)\n",
    "                    break\n",
    "        self.raw_data_set = []\n",
    "        if is_train:\n",
    "            train_point_path = \"sample_results/train_256_uniform.json\"\n",
    "        else:\n",
    "            train_point_path = \"sample_results/val_256_uniform.json\"\n",
    "        with open(train_point_path, \"r\") as f:\n",
    "            points = json.load(f)\n",
    "        for video_idx, video_data in enumerate(raw_set.data_set):\n",
    "            if video_idx in empty_video_idx:\n",
    "                continue\n",
    "            self.raw_data_set.append([])\n",
    "            for frame_idx, frame_data in enumerate(video_data):\n",
    "                img, mask = frame_data\n",
    "                point_data = points[video_idx][frame_idx][str(point_num)][\"boundary\"]\n",
    "                point = torch.tensor(point_data)\n",
    "                self.raw_data_set[-1].append((img, mask, point))\n",
    "        self.data = []\n",
    "        for video_idx, video_data in enumerate(self.raw_data_set):\n",
    "            for frame_idx in range(len(video_data) - 1):\n",
    "                self.data.append(\n",
    "                    (\n",
    "                        video_idx,\n",
    "                        frame_idx,\n",
    "                        video_data[0],\n",
    "                        video_data[frame_idx],\n",
    "                        video_data[frame_idx + 1],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        video_idx, frame_idx, first_frame, previous_frame, current_frame = self.data[\n",
    "            idx\n",
    "        ]\n",
    "        return video_idx, frame_idx, first_frame, previous_frame, current_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_train_dataset = DAVIS_withPoint(\n",
    "    train_rawset,\n",
    "    point_num=64,\n",
    "    is_train=True,\n",
    ")\n",
    "video_val_dataset = DAVIS_withPoint(\n",
    "    val_rawset,\n",
    "    point_num=64,\n",
    "    is_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rawset = copy.copy(train_rawset)\n",
    "test_rawset.data_set = train_rawset.data_set[:10]\n",
    "len(test_rawset.data_set), test_rawset.data_set[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_test_dataset = DAVIS_withPoint(\n",
    "    test_rawset,\n",
    "    point_num=64,\n",
    "    is_train=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_test_loader = DataLoader(video_test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idx, frame_idx, first_frame, previous_frame, current_frame = next(\n",
    "    iter(video_test_loader)\n",
    ")\n",
    "first_frame[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the test loader\n",
    "video_idx, frame_idx, first_frame, previous_frame, current_frame = next(\n",
    "    iter(video_test_loader)\n",
    ")\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(normalize(first_frame[0][0]).permute(1, 2, 0))\n",
    "plt.imshow(first_frame[1][0], alpha=0.5)\n",
    "plt.plot(first_frame[2][0][:, 0], first_frame[2][0][:, 1], \"r\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(normalize(previous_frame[0][0]).permute(1, 2, 0))\n",
    "plt.imshow(previous_frame[1][0], alpha=0.5)\n",
    "plt.plot(previous_frame[2][0][:, 0], previous_frame[2][0][:, 1], \"r\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(normalize(current_frame[0][0]).permute(1, 2, 0))\n",
    "plt.imshow(current_frame[1][0], alpha=0.5)\n",
    "plt.plot(current_frame[2][0][:, 0], current_frame[2][0][:, 1], \"r\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(video_train_dataset), len(video_val_dataset), len(video_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_train_dataset.raw_data_set), len(video_val_dataset.raw_data_set), len(video_test_dataset.raw_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_train_loader = DataLoader(video_train_dataset, batch_size=1, shuffle=True)\n",
    "video_val_loader = DataLoader(video_val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idx, frame_idx, first_frame, previous_frame, current_frame = next(iter(video_train_loader))\n",
    "first_frame[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the video train dataset\n",
    "video_idx, frame_idx, first_frame, previous_frame, current_frame = next(iter(video_train_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(normalize(first_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(first_frame[1][0], alpha=0.5)\n",
    "plt.plot(first_frame[2][0][:, 0], first_frame[2][0][:, 1], \"r\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(normalize(previous_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(previous_frame[1][0], alpha=0.5)\n",
    "plt.plot(previous_frame[2][0][:, 0], previous_frame[2][0][:, 1], \"r\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(normalize(current_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(current_frame[1][0], alpha=0.5)\n",
    "plt.plot(current_frame[2][0][:, 0], current_frame[2][0][:, 1], \"r\")\n",
    "plt.show()\n",
    "# test the video val dataset\n",
    "video_idx, frame_idx, first_frame, previous_frame, current_frame = next(iter(video_val_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(normalize(first_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(first_frame[1][0], alpha=0.5)\n",
    "plt.plot(first_frame[2][0][:, 0], first_frame[2][0][:, 1], \"r\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(normalize(previous_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(previous_frame[1][0], alpha=0.5)\n",
    "plt.plot(previous_frame[2][0][:, 0], previous_frame[2][0][:, 1], \"r\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(normalize(current_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(current_frame[1][0], alpha=0.5)\n",
    "plt.plot(current_frame[2][0][:, 0], current_frame[2][0][:, 1], \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableTransformerExtraDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        d_ffn=1024,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        n_levels=4,\n",
    "        n_heads=8,\n",
    "        n_points=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # extra cross attention\n",
    "        self.extra_cross_attn = nn.MultiheadAttention(\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.extra_norm = nn.LayerNorm(d_model)\n",
    "        self.extra_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # cross attention\n",
    "        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout4(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        query_pos,\n",
    "        reference_points,\n",
    "        src,\n",
    "        src_spatial_shapes,\n",
    "        level_start_index,\n",
    "        extra_memory,\n",
    "        src_padding_mask=None,\n",
    "    ):\n",
    "        # self attention\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(\n",
    "            q.transpose(0, 1), k.transpose(0, 1), tgt.transpose(0, 1)\n",
    "        )[0].transpose(0, 1)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # extra cross attention\n",
    "        tgt_extra = self.extra_cross_attn(\n",
    "            self.with_pos_embed(tgt, query_pos),\n",
    "            extra_memory,\n",
    "            extra_memory,\n",
    "        )[0]\n",
    "        tgt = tgt + self.extra_dropout(tgt_extra)\n",
    "        tgt = self.extra_norm(tgt)\n",
    "\n",
    "        # cross attention\n",
    "        tgt2 = self.cross_attn(\n",
    "            self.with_pos_embed(tgt, query_pos),\n",
    "            reference_points,\n",
    "            src,\n",
    "            src_spatial_shapes,\n",
    "            level_start_index,\n",
    "            src_padding_mask,\n",
    "        )\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # ffn\n",
    "        tgt = self.forward_ffn(tgt)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class DeformableTransformerExtraDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        reference_points,\n",
    "        src,\n",
    "        src_spatial_shapes,\n",
    "        src_level_start_index,\n",
    "        src_valid_ratios,\n",
    "        extra_memory,\n",
    "        query_pos=None,\n",
    "        src_padding_mask=None,\n",
    "    ):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "        intermediate_reference_points = []\n",
    "        for lid, layer in enumerate(self.layers):\n",
    "            if reference_points.shape[-1] == 4:\n",
    "                reference_points_input = (\n",
    "                    reference_points[:, :, None]\n",
    "                    * torch.cat([src_valid_ratios, src_valid_ratios], -1)[:, None]\n",
    "                )\n",
    "            else:\n",
    "                assert reference_points.shape[-1] == 2\n",
    "                reference_points_input = (\n",
    "                    reference_points[:, :, None] * src_valid_ratios[:, None]\n",
    "                )\n",
    "            output = layer(\n",
    "                output,\n",
    "                query_pos,\n",
    "                reference_points_input,\n",
    "                src,\n",
    "                src_spatial_shapes,\n",
    "                src_level_start_index,\n",
    "                extra_memory,\n",
    "                src_padding_mask,\n",
    "            )\n",
    "\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "                intermediate_reference_points.append(reference_points)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n",
    "\n",
    "        return output, reference_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the video train dataset\n",
    "video_idx, frame_idx, first_frame, previous_frame, current_frame = next(iter(video_train_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(normalize(first_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(first_frame[1][0], alpha=0.5)\n",
    "plt.plot(first_frame[2][0][:, 0], first_frame[2][0][:, 1], \"r\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(normalize(previous_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(previous_frame[1][0], alpha=0.5)\n",
    "plt.plot(previous_frame[2][0][:, 0], previous_frame[2][0][:, 1], \"r\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(normalize(current_frame[0][0]).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(current_frame[1][0], alpha=0.5)\n",
    "plt.plot(current_frame[2][0][:, 0], current_frame[2][0][:, 1], \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fir_img, fir_mask, fir_point = first_frame\n",
    "pre_img, pre_mask, pre_point = previous_frame\n",
    "cur_img, cur_mask, cur_point = current_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformVideo(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_num=1,\n",
    "        up_scale_num=4,\n",
    "        head_num=6,\n",
    "        medium_level_size=[14, 28, 56, 112],\n",
    "        offset_limit=56,\n",
    "        n_points=4,\n",
    "        mem_point_num=64,\n",
    "        freeze_backbone=True,\n",
    "    ) -> None:\n",
    "        super(DeformVideo, self).__init__()\n",
    "        self.up_scale_num = up_scale_num\n",
    "        self.offset_limit = offset_limit\n",
    "        self.medium_level_size = medium_level_size\n",
    "        self.featup = torch.hub.load(\n",
    "            \"mhamilton723/FeatUp\",\n",
    "            \"dino16\",\n",
    "            use_norm=True,\n",
    "        ).cuda()\n",
    "        if freeze_backbone:\n",
    "            for param in self.featup.parameters():\n",
    "                param.requires_grad = False\n",
    "        d_model = 384\n",
    "        d_ffn = 1024\n",
    "        n_levels = len(medium_level_size) + 1\n",
    "        self.pos_enoc = PositionalEncoding(d_model)\n",
    "\n",
    "        enc_layer = DeformableTransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            d_ffn=d_ffn,\n",
    "            n_levels=n_levels,\n",
    "            n_heads=head_num,\n",
    "            n_points=n_points,\n",
    "        )\n",
    "        dec_layer = DeformableTransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            d_ffn=d_ffn,\n",
    "            n_levels=n_levels,\n",
    "            n_heads=head_num,\n",
    "            n_points=n_points,\n",
    "        )\n",
    "\n",
    "        self.first_query_embed = nn.Embedding(mem_point_num, d_model)\n",
    "        xavier_uniform_(self.first_query_embed.weight)\n",
    "        self.first_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.fir_enc = DeformableTransformerEncoder(\n",
    "            enc_layer,\n",
    "            num_layers=layer_num,\n",
    "        )\n",
    "        self.fir_dec = DeformableTransformerDecoder(\n",
    "            dec_layer,\n",
    "            num_layers=layer_num,\n",
    "        )\n",
    "        self.previous_query_embed = nn.Embedding(mem_point_num, d_model)\n",
    "        xavier_uniform_(self.previous_query_embed.weight)\n",
    "        self.previous_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.pre_enc = DeformableTransformerEncoder(\n",
    "            enc_layer,\n",
    "            num_layers=layer_num,\n",
    "        )\n",
    "        self.pre_dec = DeformableTransformerDecoder(\n",
    "            dec_layer,\n",
    "            num_layers=layer_num,\n",
    "        )\n",
    "        self.extra_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        query_num = 4\n",
    "        for _ in range(up_scale_num):\n",
    "            query_num *= 2\n",
    "        self.current_query_embed = nn.Embedding(query_num, d_model)\n",
    "        xavier_uniform_(self.current_query_embed.weight)\n",
    "        self.current_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.cur_enc = DeformableTransformerEncoder(\n",
    "            enc_layer,\n",
    "            num_layers=layer_num,\n",
    "        )\n",
    "        extra_dec_layer = DeformableTransformerExtraDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            d_ffn=d_ffn,\n",
    "            n_levels=n_levels,\n",
    "            n_heads=head_num,\n",
    "            n_points=n_points,\n",
    "        )\n",
    "        extra_dec = DeformableTransformerExtraDecoder(\n",
    "            extra_dec_layer,\n",
    "            num_layers=layer_num,\n",
    "        )\n",
    "        self.extra_decs = _get_clones(extra_dec, up_scale_num + 1)\n",
    "        xy_fc = MLP(d_model, d_model, 2, 3)\n",
    "        self.xy_fcs = _get_clones(xy_fc, up_scale_num + 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        fir_img: torch.Tensor,\n",
    "        fir_bou: torch.Tensor,\n",
    "        pre_img: torch.Tensor,\n",
    "        pre_bou: torch.Tensor,\n",
    "        pre_sgm: torch.Tensor,\n",
    "        cur_img: torch.Tensor,\n",
    "    ):\n",
    "        fir_bou = fir_bou / 224\n",
    "        pre_bou = pre_bou / 224\n",
    "        (\n",
    "            fir_img_srcs_flatten,\n",
    "            fir_spatial_shapes,\n",
    "            fir_level_start_index,\n",
    "            fir_valid_ratios,\n",
    "        ) = self._get_enced_img_scrs(\n",
    "            fir_img,\n",
    "            self.fir_enc,\n",
    "            self.first_layer_norm,\n",
    "        )\n",
    "        (\n",
    "            pre_img_srcs_flatten,\n",
    "            pre_spatial_shapes,\n",
    "            pre_level_start_index,\n",
    "            pre_valid_ratios,\n",
    "        ) = self._get_enced_img_scrs(\n",
    "            pre_img,\n",
    "            self.pre_enc,\n",
    "            self.previous_layer_norm,\n",
    "        )\n",
    "        (\n",
    "            cur_img_srcs_flatten,\n",
    "            cur_spatial_shapes,\n",
    "            cur_level_start_index,\n",
    "            cur_valid_ratios,\n",
    "        ) = self._get_enced_img_scrs(\n",
    "            cur_img,\n",
    "            self.cur_enc,\n",
    "            self.current_layer_norm,\n",
    "        )\n",
    "\n",
    "        B, S, C = fir_img_srcs_flatten.shape\n",
    "        first_queries = (\n",
    "            self.first_query_embed.weight.unsqueeze(0).repeat(B, 1, 1).cuda()\n",
    "        )\n",
    "        first_memory, _ = self.fir_dec(\n",
    "            first_queries,\n",
    "            fir_bou,\n",
    "            fir_img_srcs_flatten,\n",
    "            fir_spatial_shapes,\n",
    "            fir_level_start_index,\n",
    "            fir_valid_ratios,\n",
    "        )\n",
    "        previous_queries = (\n",
    "            self.previous_query_embed.weight.unsqueeze(0).repeat(B, 1, 1).cuda()\n",
    "        )\n",
    "        previous_memory, _ = self.pre_dec(\n",
    "            previous_queries,\n",
    "            pre_bou,\n",
    "            pre_img_srcs_flatten,\n",
    "            pre_spatial_shapes,\n",
    "            pre_level_start_index,\n",
    "            pre_valid_ratios,\n",
    "        )\n",
    "        extra_memory = torch.cat([first_memory, previous_memory], 1)\n",
    "        extra_memory = self.extra_layer_norm(extra_memory)\n",
    "        extra_memory = self.pos_enoc(extra_memory)\n",
    "\n",
    "        cur_queries = (\n",
    "            self.current_query_embed.weight.unsqueeze(0).repeat(B, 1, 1).cuda()\n",
    "        )\n",
    "        init_bou = get_bounding_box(pre_sgm).cuda() / 224\n",
    "        current_query_num = init_bou.shape[1]\n",
    "        current_query = cur_queries[:, :current_query_num]\n",
    "        decode_output, _ = self.extra_decs[0](\n",
    "            current_query,\n",
    "            init_bou,\n",
    "            cur_img_srcs_flatten,\n",
    "            cur_spatial_shapes,\n",
    "            cur_level_start_index,\n",
    "            cur_valid_ratios,\n",
    "            extra_memory,\n",
    "        )\n",
    "\n",
    "        xy_offset = (\n",
    "            (self.xy_fcs[0](decode_output).sigmoid() - 0.5) * self.offset_limit / 224\n",
    "        )\n",
    "        init_bou += xy_offset\n",
    "\n",
    "        results = [init_bou]\n",
    "        for i in range(self.up_scale_num):\n",
    "            new_query = cur_queries[:, current_query_num : current_query_num * 2]\n",
    "            current_query_num *= 2\n",
    "\n",
    "            current_query = torch.zeros((B, current_query_num, C)).to(\n",
    "                cur_img_srcs_flatten.device\n",
    "            )\n",
    "            current_query[:, ::2] = new_query\n",
    "            current_query[:, 1::2] = decode_output\n",
    "\n",
    "            cur_bou = add_mid_points(results[-1])\n",
    "            decode_output, _ = self.extra_decs[i + 1](\n",
    "                current_query,\n",
    "                cur_bou,\n",
    "                cur_img_srcs_flatten,\n",
    "                cur_spatial_shapes,\n",
    "                cur_level_start_index,\n",
    "                cur_valid_ratios,\n",
    "                extra_memory,\n",
    "            )\n",
    "\n",
    "            xy_offset = (\n",
    "                (self.xy_fcs[i + 1](decode_output).sigmoid() - 0.5)\n",
    "                * self.offset_limit\n",
    "                / 224\n",
    "            )\n",
    "            cur_bou += xy_offset\n",
    "\n",
    "            results.append(cur_bou)\n",
    "        results = [result * 224 for result in results]\n",
    "\n",
    "        if self.training:\n",
    "            return results\n",
    "        else:\n",
    "            result = results[-1]\n",
    "            result = result.clamp(0, 223)\n",
    "            return result\n",
    "\n",
    "    def _get_img_scrs(self, img: torch.Tensor, layernorm: nn.LayerNorm):\n",
    "        feats = self.featup(img)\n",
    "        srcs = []\n",
    "        padding_masks = []\n",
    "        for low_res in self.medium_level_size:\n",
    "            srcs.append(\n",
    "                F.interpolate(\n",
    "                    feats,\n",
    "                    size=(low_res, low_res),\n",
    "                    mode=\"bilinear\",\n",
    "                ),\n",
    "            )\n",
    "        srcs.append(feats)\n",
    "        for src in srcs:\n",
    "            padding_masks.append(torch.zeros_like(src[:, 0:1, :, :]).squeeze(1).bool())\n",
    "        src_flatten = []\n",
    "        spatial_shapes = []\n",
    "        for src in srcs:\n",
    "            src_flatten.append(\n",
    "                rearrange(src, \"b c h w -> b (h w) c\"),\n",
    "            )\n",
    "            spatial_shapes.append(src.shape[-2:])\n",
    "        level_start_index = torch.cat(\n",
    "            (\n",
    "                torch.tensor([0]),\n",
    "                torch.cumsum(\n",
    "                    torch.tensor([x.shape[1] for x in src_flatten]),\n",
    "                    0,\n",
    "                )[:-1],\n",
    "            )\n",
    "        ).cuda()\n",
    "        src_flatten = torch.cat(src_flatten, 1).cuda()\n",
    "        valid_ratios = torch.stack(\n",
    "            [get_valid_ratio(mask) for mask in padding_masks],\n",
    "            1,\n",
    "        ).cuda()\n",
    "        spatial_shapes = torch.as_tensor(\n",
    "            spatial_shapes,\n",
    "            dtype=torch.long,\n",
    "            device=src_flatten.device,\n",
    "        )\n",
    "        src_flatten = layernorm(src_flatten)\n",
    "        src_flatten = self.pos_enoc(src_flatten)\n",
    "        return src_flatten, spatial_shapes, level_start_index, valid_ratios\n",
    "\n",
    "    def _get_enced_img_scrs(\n",
    "        self,\n",
    "        img: torch.Tensor,\n",
    "        encoder: DeformableTransformerEncoder,\n",
    "        layernorm: nn.LayerNorm,\n",
    "    ):\n",
    "        src_flatten, spatial_shapes, level_start_index, valid_ratios = (\n",
    "            self._get_img_scrs(img, layernorm)\n",
    "        )\n",
    "        src_flatten = encoder(\n",
    "            src=src_flatten,\n",
    "            spatial_shapes=spatial_shapes,\n",
    "            level_start_index=level_start_index,\n",
    "            valid_ratios=valid_ratios,\n",
    "        )\n",
    "        return src_flatten, spatial_shapes, level_start_index, valid_ratios\n",
    "\n",
    "\n",
    "model = DeformVideo().cuda()\n",
    "model.eval()\n",
    "results = model(\n",
    "    fir_img.cuda(),\n",
    "    fir_point.cuda(),\n",
    "    pre_img.cuda(),\n",
    "    pre_point.cuda(),\n",
    "    pre_mask.cuda(),\n",
    "    cur_img.cuda(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "results = model(\n",
    "    fir_img.cuda(),\n",
    "    fir_point.cuda(),\n",
    "    pre_img.cuda(),\n",
    "    pre_point.cuda(),\n",
    "    pre_mask.cuda(),\n",
    "    cur_img.cuda(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeformVideo().cuda()\n",
    "loss_dict = {}\n",
    "iou_test_dict = {}\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epoch_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epoch_num):\n",
    "    mean_loss = 0\n",
    "    train_mean_iou = 0\n",
    "    for video_idx, frame_idx, first_frame, previous_frame, current_frame in tqdm(\n",
    "        video_test_loader\n",
    "    ):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        fir_img, fir_mask, fir_point = first_frame\n",
    "        pre_img, pre_mask, pre_point = previous_frame\n",
    "        cur_img, cur_mask, cur_point = current_frame\n",
    "        results = model(\n",
    "            fir_img.cuda(),\n",
    "            fir_point.cuda(),\n",
    "            pre_img.cuda(),\n",
    "            pre_point.cuda(),\n",
    "            pre_mask.cuda(),\n",
    "            cur_img.cuda(),\n",
    "        )\n",
    "        loss = 0\n",
    "        for result in results:\n",
    "            loss += ras_loss(result, cur_mask.cuda())\n",
    "        loss /= len(results)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_loss += loss.item()\n",
    "        iou = get_batch_average_bou_iou(results[-1], cur_mask.cuda(), gt_rasterizer)\n",
    "        train_mean_iou += iou.item()\n",
    "    mean_loss /= len(video_test_loader)\n",
    "    train_mean_iou /= len(video_test_loader)\n",
    "    loss_dict[e] = mean_loss\n",
    "    iou_train_dict[e] = train_mean_iou\n",
    "    print(f\"Epoch {e} train loss: {mean_loss:.4f}, iou: {train_mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "model.eval()\n",
    "video_idx, frame_idx, first_frame, previous_frame, current_frame = next(\n",
    "    iter(video_test_loader)\n",
    ")\n",
    "fir_img, fir_mask, fir_point = first_frame\n",
    "pre_img, pre_mask, pre_point = previous_frame\n",
    "cur_img, cur_mask, cur_point = current_frame\n",
    "result = model(\n",
    "    fir_img.cuda(),\n",
    "    fir_point.cuda(),\n",
    "    pre_img.cuda(),\n",
    "    pre_point.cuda(),\n",
    "    pre_mask.cuda(),\n",
    "    cur_img.cuda(),\n",
    ")\n",
    "np_result = result[0].detach().cpu().numpy()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(normalize(cur_img[0]).permute(1, 2, 0))\n",
    "plt.imshow(cur_mask[0], alpha=0.5)\n",
    "plt.plot(np_result[:, 0], np_result[:, 1], \"r\")\n",
    "plt.scatter(np_result[:, 0], np_result[:, 1], c=\"r\", s=5)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cur_mask[0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoInferer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: DAVIS_withPoint,\n",
    "        gt_rasterizer: SoftPolygon,\n",
    "    ) -> None:\n",
    "        self.data_set = dataset.raw_data_set\n",
    "        self.gt_rasterizer = gt_rasterizer\n",
    "\n",
    "    def infer_one_video(self, video_idx: int, model: nn.Module):\n",
    "        infer_results = []\n",
    "        video_data = self.data_set[video_idx]\n",
    "        model.eval()\n",
    "        fir_img, fir_mask, fir_point = video_data[0]\n",
    "        pre_img, pre_mask, pre_point = video_data[0]\n",
    "        fir_img = fir_img.unsqueeze(0)\n",
    "        pre_img = pre_img.unsqueeze(0)\n",
    "        fir_mask = fir_mask.unsqueeze(0)\n",
    "        pre_mask = pre_mask.unsqueeze(0)\n",
    "        fir_point = fir_point.unsqueeze(0)\n",
    "        pre_point = pre_point.unsqueeze(0)\n",
    "        infer_results.append(None)\n",
    "        with torch.no_grad():\n",
    "            for i in range(1, len(video_data)):\n",
    "                cur_img, cur_mask, cur_point = video_data[i]\n",
    "                cur_img = cur_img.unsqueeze(0)\n",
    "                cur_mask = cur_mask.unsqueeze(0)\n",
    "                cur_point = cur_point.unsqueeze(0)\n",
    "                pred_bou = model(\n",
    "                    fir_img.cuda(),\n",
    "                    fir_point.cuda(),\n",
    "                    pre_img.cuda(),\n",
    "                    pre_point.cuda(),\n",
    "                    pre_mask.cuda(),\n",
    "                    cur_img.cuda(),\n",
    "                )\n",
    "                pred_mask = self.gt_rasterizer(pred_bou, 224, 224)\n",
    "                pred_mask[pred_mask == -1] = 0\n",
    "                iou = get_batch_average_bou_iou(\n",
    "                    pred_bou, cur_mask.cuda(), self.gt_rasterizer\n",
    "                )\n",
    "                infer_results.append((pred_bou, pred_mask, iou.item()))\n",
    "                pre_img, pre_mask, pre_point = cur_img, pred_mask, pred_bou\n",
    "        return infer_results\n",
    "\n",
    "    def infer_all_videos(self, model: nn.Module):\n",
    "        self.infer_results = []\n",
    "        for video_idx in tqdm(range(len(self.data_set))):\n",
    "            infer_results = self.infer_one_video(video_idx, model)\n",
    "            self.infer_results.append(infer_results)\n",
    "\n",
    "    def compute_video_iou(self, video_idx: int):\n",
    "        infer_results = self.infer_results[video_idx]\n",
    "        ious = [result[-1] for result in infer_results[1:]]\n",
    "        return np.mean(ious)\n",
    "\n",
    "    def compute_all_videos_iou(self):\n",
    "        self.video_ious = []\n",
    "        for video_idx in range(len(self.data_set)):\n",
    "            iou = self.compute_video_iou(video_idx)\n",
    "            self.video_ious.append(iou)\n",
    "        # return the average iou\n",
    "        return np.mean(self.video_ious)\n",
    "\n",
    "    def show_video_results(\n",
    "        self,\n",
    "        video_idx: int,\n",
    "        mask_alpha=0.2,\n",
    "        img_per_line=5,\n",
    "    ):\n",
    "        video_data = self.data_set[video_idx]\n",
    "        pred_results = self.infer_results[video_idx]\n",
    "        frame_num = len(video_data)\n",
    "        line_num = frame_num // img_per_line + 1\n",
    "        plt.figure(figsize=(20, 4 * line_num))\n",
    "        for i, pred_data in enumerate(pred_results):\n",
    "            plt.subplot(line_num, img_per_line, i + 1)\n",
    "            cur_img, cur_mask, cur_point = video_data[i]\n",
    "            plt.imshow(normalize(cur_img).permute(1, 2, 0))\n",
    "            plt.imshow(cur_mask, alpha=mask_alpha)\n",
    "            if pred_data is None:\n",
    "                plt.title(\"ground truth\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.plot(cur_point[:, 0], cur_point[:, 1], \"r\")\n",
    "                plt.scatter(cur_point[:, 0], cur_point[:, 1], c=\"r\", s=5)\n",
    "            else:\n",
    "                pred_bou, pred_mask, iou = pred_data\n",
    "                plt.title(f\"iou: {iou:.4f}\")\n",
    "                plt.axis(\"off\")\n",
    "                pred_bou = pred_bou[0].detach().cpu().numpy()\n",
    "                plt.plot(pred_bou[:, 0], pred_bou[:, 1], \"r\")\n",
    "                plt.scatter(pred_bou[:, 0], pred_bou[:, 1], c=\"r\", s=5)\n",
    "        \n",
    "\n",
    "# video_inferer = VideoInferer(video_test_dataset, gt_rasterizer)\n",
    "# video_inferer.infer_all_videos(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_train_rawset = copy.copy(train_rawset)\n",
    "short_train_rawset.data_set = train_rawset.data_set[:5]\n",
    "short_video_train_dataset = DAVIS_withPoint(\n",
    "    short_train_rawset,\n",
    "    point_num=64,\n",
    "    is_train=True,\n",
    ")\n",
    "len(short_video_train_dataset.raw_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_val_rawset = copy.copy(val_rawset)\n",
    "short_val_rawset.data_set = val_rawset.data_set[:2]\n",
    "short_video_val_dataset = DAVIS_withPoint(\n",
    "    short_val_rawset,\n",
    "    point_num=64,\n",
    "    is_train=False,\n",
    ")\n",
    "# short_video_val_dataset.raw_data_set = short_video_val_dataset.raw_data_set[6:9]\n",
    "# len(short_video_val_dataset.raw_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_train_dataset), len(video_val_dataset), len(short_video_train_dataset), len(short_video_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inferer = VideoInferer(short_video_train_dataset, gt_rasterizer)\n",
    "short_train_loader = DataLoader(short_video_train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeformVideo().cuda()\n",
    "loss_dict = {}\n",
    "iou_train_dict = {}\n",
    "iou_val_dict = {}\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epoch_num = 9\n",
    "eval_period = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epoch_num):\n",
    "    mean_loss = 0\n",
    "    train_mean_iou = 0\n",
    "    for video_idx, frame_idx, first_frame, previous_frame, current_frame in tqdm(\n",
    "        short_train_loader\n",
    "    ):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        fir_img, fir_mask, fir_point = first_frame\n",
    "        pre_img, pre_mask, pre_point = previous_frame\n",
    "        cur_img, cur_mask, cur_point = current_frame\n",
    "        results = model(\n",
    "            fir_img.cuda(),\n",
    "            fir_point.cuda(),\n",
    "            pre_img.cuda(),\n",
    "            pre_point.cuda(),\n",
    "            pre_mask.cuda(),\n",
    "            cur_img.cuda(),\n",
    "        )\n",
    "        loss = 0\n",
    "        for result in results:\n",
    "            loss += ras_loss(result, cur_mask.cuda())\n",
    "        loss /= len(results)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_loss += loss.item()\n",
    "        iou = get_batch_average_bou_iou(results[-1], cur_mask.cuda(), gt_rasterizer)\n",
    "        train_mean_iou += iou.item()\n",
    "    mean_loss /= len(short_train_loader)\n",
    "    train_mean_iou /= len(short_train_loader)\n",
    "    loss_dict[e] = mean_loss\n",
    "    iou_train_dict[e] = train_mean_iou\n",
    "    print(f\"Epoch {e} train loss: {mean_loss:.4f}, iou: {train_mean_iou:.4f}\")\n",
    "    if e % eval_period == 0 or e == epoch_num - 1:\n",
    "        val_inferer.infer_all_videos(model)\n",
    "        val_iou = val_inferer.compute_all_videos_iou()\n",
    "        iou_val_dict[e] = val_iou\n",
    "        print(f\"Epoch {e} val iou: {val_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inferer.infer_all_videos(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inferer.compute_all_videos_iou()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inferer.video_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inferer.show_video_results(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and check the results\n",
    "model_name = \"deform_video\"\n",
    "log_dir = f\"./log/{model_name}\"\n",
    "log_path = f\"{log_dir}/{model_name}.log\"\n",
    "model_path = f\"./model/{model_name}_best.pth\"\n",
    "# load the log\n",
    "with open(log_path, \"r\") as f:\n",
    "    logs = f.readlines()\n",
    "loss_dict = {}\n",
    "iou_train_dict = {}\n",
    "iou_val_dict = {}\n",
    "# get the loss and iou using regex\n",
    "for log in logs:\n",
    "    # get the epoch\n",
    "    epoch = re.search(r\"Epoch (\\d+)\", log)\n",
    "    if epoch is not None:\n",
    "        epoch = int(epoch.group(1))\n",
    "        # get the loss\n",
    "        loss = re.search(r\"train loss: ([\\d.]+)\", log)\n",
    "        if loss is not None:\n",
    "            loss_dict[epoch] = float(loss.group(1))\n",
    "        # get the train iou\n",
    "        iou = re.search(r\"train iou: ([\\d.]+)\", log)\n",
    "        if iou is not None:\n",
    "            iou_train_dict[epoch] = float(iou.group(1))\n",
    "        # get the val iou\n",
    "        iou = re.search(r\"val iou: ([\\d.]+)\", log)\n",
    "        if iou is not None:\n",
    "            iou_val_dict[epoch] = float(iou.group(1))\n",
    "# plot the loss and iou\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.grid()\n",
    "plt.plot(loss_dict.keys(), loss_dict.values())\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"train loss\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.grid()\n",
    "plt.plot(iou_train_dict.keys(), iou_train_dict.values(), label=\"train iou\")\n",
    "plt.plot(iou_val_dict.keys(), iou_val_dict.values(), label=\"val iou\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"iou\")\n",
    "plt.title(\"iou\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and check the results\n",
    "model_name = \"def_davis_std\"\n",
    "log_dir = f\"./log/{model_name}\"\n",
    "log_path = f\"{log_dir}/{model_name}.log\"\n",
    "model_path = f\"./model/{model_name}_best.pth\"\n",
    "# load the log\n",
    "with open(log_path, \"r\") as f:\n",
    "    logs = f.readlines()\n",
    "dif_loss_dict = {}\n",
    "std_loss_dict = {}\n",
    "iou_train_dict = {}\n",
    "iou_val_dict = {}\n",
    "# # get the loss and iou using regex\n",
    "# for log in logs:\n",
    "#     # get the epoch\n",
    "#     epoch = re.search(r\"Epoch (\\d+)\", log)\n",
    "#     if epoch is not None:\n",
    "#         epoch = int(epoch.group(1))\n",
    "#         # get the dif loss\n",
    "#         dif_loss = re.search(r\"dif loss: ([\\d.]+)\", log)\n",
    "#         if dif_loss is not None:\n",
    "#             dif_loss_dict[epoch] = float(dif_loss.group(1))\n",
    "#         # get the std loss\n",
    "#         std_loss = re.search(r\"std loss: ([\\d.]+)\", log)\n",
    "#         if std_loss is not None:\n",
    "#             std_loss_dict[epoch] = float(std_loss.group(1))\n",
    "#         # get the train iou\n",
    "#         iou = re.search(r\"train iou: ([\\d.]+)\", log)\n",
    "#         if iou is not None:\n",
    "#             iou_train_dict[epoch] = float(iou.group(1))\n",
    "#         # get the val iou\n",
    "#         iou = re.search(r\"val iou: ([\\d.]+)\", log)\n",
    "#         if iou is not None:\n",
    "#             iou_val_dict[epoch] = float(iou.group(1))\n",
    "with open(f\"{log_dir}/dif_loss.json\", \"r\") as f:\n",
    "    dif_loss_dict = json.load(f)\n",
    "with open(f\"{log_dir}/std_loss.json\", \"r\") as f:\n",
    "    std_loss_dict = json.load(f)\n",
    "with open(f\"{log_dir}/iou_train.json\", \"r\") as f:\n",
    "    iou_train_dict = json.load(f)\n",
    "with open(f\"{log_dir}/iou_val.json\", \"r\") as f:\n",
    "    iou_val_dict = json.load(f)\n",
    "# plot the loss and iou\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.grid()\n",
    "plt.plot(dif_loss_dict.keys(), dif_loss_dict.values(), label=\"dif loss\")\n",
    "plt.plot(std_loss_dict.keys(), std_loss_dict.values(), label=\"std loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.title(\"train loss\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.grid()\n",
    "plt.plot(iou_train_dict.keys(), iou_train_dict.values(), label=\"train iou\")\n",
    "plt.plot(iou_val_dict.keys(), iou_val_dict.values(), label=\"val iou\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"iou\")\n",
    "plt.title(\"iou\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = DeformVideo().cuda()\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inferer = VideoInferer(video_train_dataset, gt_rasterizer)\n",
    "val_inferer = VideoInferer(video_val_dataset, gt_rasterizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inferer.infer_all_videos(model)\n",
    "val_inferer.infer_all_videos(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inferer.compute_all_videos_iou(), val_inferer.compute_all_videos_iou()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the ious\n",
    "for i, iou in enumerate(train_inferer.video_ious):\n",
    "    print(f\"train video {i} iou: {iou:.4f}\")\n",
    "for i, iou in enumerate(val_inferer.video_ious):\n",
    "    print(f\"val video {i} iou: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train iou distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(train_inferer.video_ious, bins=10)\n",
    "plt.title(\"train iou distribution\")\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "plt.show()\n",
    "# plot the validation iou distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(val_inferer.video_ious, bins=10)\n",
    "plt.title(\"val iou distribution\")\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_check_idxs = [0, 100, 46, 55, 96, 1, 41, 21, 28, 51, 76, 93]\n",
    "for idx in train_check_idxs:\n",
    "    train_inferer.show_video_results(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_check_idxs = [2, 7, 9, 10, 18, 24, 13, 41, 46, 50, 54, 39]\n",
    "for idx in val_check_idxs:\n",
    "    val_inferer.show_video_results(idx)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
