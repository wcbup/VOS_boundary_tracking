{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from model import PositionalEncoding, get_bou_features, find_best_shift, get_img_tokens\n",
    "from model import IterWholeFirst\n",
    "from einops import rearrange\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from featup.util import norm, unnorm\n",
    "from featup.plotting import plot_feats\n",
    "from collections import OrderedDict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DETR_model import get_res4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "res4 = get_res4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 14, 14])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.rand(3, 4, 224, 224).cuda()\n",
    "res4(tmp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_num = 80\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fir_img = torch.rand(3, 3, 224, 224).to(device)\n",
    "fir_mask = torch.rand(3, 1, 224, 224).to(device)\n",
    "fir_con = torch.cat((fir_img, fir_mask), dim=1)\n",
    "pre_img = torch.rand(3, 3, 224, 224).to(device)\n",
    "pre_mask = torch.rand(3, 1, 224, 224).to(device)\n",
    "pre_con = torch.cat((pre_img, pre_mask), dim=1)\n",
    "cur_img = torch.rand(3, 3, 224, 224).to(device)\n",
    "cur_mask = torch.rand(3, 1, 224, 224).to(device)\n",
    "pre_bou = torch.randint(0, 224, (3, boundary_num, 2)).to(device)\n",
    "fir_bou = torch.randint(0, 224, (3, boundary_num, 2)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = fir_img * fir_mask.squeeze()\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "featup = torch.hub.load(\n",
    "    \"mhamilton723/FeatUp\",\n",
    "    \"dino16\",\n",
    "    use_norm=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_channels(conv2d: nn.Conv2d, extra_chan=1):\n",
    "    \"\"\"\n",
    "    Add extra channels to a Conv2d layer.\n",
    "    \"\"\"\n",
    "    device = conv2d.weight.device\n",
    "    new_conv2d = nn.Conv2d(\n",
    "        conv2d.in_channels + extra_chan,\n",
    "        conv2d.out_channels,\n",
    "        conv2d.kernel_size,\n",
    "        conv2d.stride,\n",
    "        conv2d.padding,\n",
    "        conv2d.dilation,\n",
    "        conv2d.groups,\n",
    "        conv2d.bias is not None,\n",
    "        conv2d.padding_mode,\n",
    "    ).to(device)\n",
    "    new_dict = OrderedDict()\n",
    "    for name, param in new_conv2d.state_dict().items():\n",
    "        new_param = conv2d.state_dict()[name]\n",
    "        if new_param.shape != param.shape:\n",
    "            c, _, w, h = param.shape\n",
    "            pads = torch.zeros((c, extra_chan, w, h)).to(device)\n",
    "            nn.init.orthogonal_(pads)\n",
    "            new_param = torch.cat((new_param, pads), dim=1)\n",
    "        new_dict[name] = new_param\n",
    "    new_conv2d.load_state_dict(new_dict)\n",
    "    return new_conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino16 = featup.model\n",
    "dino16[0].model.patch_embed.proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(4, 384, kernel_size=(16, 16), stride=(16, 16))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_proj = add_extra_channels(dino16[0].model.patch_embed.proj, 1)\n",
    "tmp_mask = torch.zeros((3, 1, 224, 224)).to(device)\n",
    "offset = new_proj(\n",
    "    torch.cat(\n",
    "        [fir_img, tmp_mask],\n",
    "        dim=1,\n",
    "    )\n",
    ") - dino16[0].model.patch_embed.proj(fir_img)\n",
    "new_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384, 14, 14])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dino4(device=\"cuda\") -> nn.Module:\n",
    "    featup = torch.hub.load(\n",
    "        \"mhamilton723/FeatUp\",\n",
    "        \"dino16\",\n",
    "        use_norm=True,\n",
    "    ).to(device)\n",
    "    dino4 = featup.model\n",
    "    new_proj = add_extra_channels(dino4[0].model.patch_embed.proj, 1)\n",
    "    dino4[0].model.patch_embed.proj = new_proj\n",
    "    return dino4\n",
    "dino4 = get_dino4()\n",
    "dino4(torch.cat([fir_img, fir_mask], dim=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 392, 384])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 384\n",
    "\n",
    "fir_con_feats = dino4(torch.cat([fir_img, fir_mask], dim=1))\n",
    "pre_con_feats = dino4(torch.cat([pre_img, pre_mask], dim=1))\n",
    "fir_con_tokens = get_img_tokens(fir_con_feats)\n",
    "pre_con_tokens = get_img_tokens(pre_con_feats)\n",
    "mem_img_tokens = torch.cat((fir_con_tokens, pre_con_tokens), dim=1)\n",
    "\n",
    "layernorm = nn.LayerNorm(hidden_dim).to(device)\n",
    "mem_img_tokens = layernorm(mem_img_tokens)\n",
    "\n",
    "pos_enc = PositionalEncoding(hidden_dim).to(device)\n",
    "mem_img_tokens = pos_enc(mem_img_tokens)\n",
    "mem_img_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80, 384])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary_num = 80\n",
    "queries = nn.Parameter(torch.rand(boundary_num, hidden_dim)).to(device)\n",
    "B, S, D = mem_img_tokens.shape\n",
    "queries = queries.unsqueeze(0).expand(B, -1, -1)\n",
    "transformer1 = nn.Transformer(\n",
    "    d_model=hidden_dim,\n",
    "    nhead=1,\n",
    "    num_encoder_layers=1,\n",
    "    num_decoder_layers=1,\n",
    "    batch_first=True,\n",
    ").to(device)\n",
    "tmp = transformer1(mem_img_tokens, queries)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_dino(device=\"cuda\") -> nn.Module:\n",
    "    featup = torch.hub.load(\n",
    "        \"mhamilton723/FeatUp\",\n",
    "        \"dino16\",\n",
    "        use_norm=True,\n",
    "    ).to(device)\n",
    "    dino = featup.model\n",
    "    return dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "raw_dino = get_raw_dino()\n",
    "cur_img_feats = raw_dino(cur_img)\n",
    "cur_img_tokens = get_img_tokens(cur_img_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 196, 384])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_img_tokens = layernorm(cur_img_tokens)\n",
    "cur_img_tokens = pos_enc(cur_img_tokens)\n",
    "cur_img_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80, 384])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer2 = nn.Transformer(\n",
    "    d_model=hidden_dim,\n",
    "    nhead=1,\n",
    "    num_encoder_layers=1,\n",
    "    num_decoder_layers=1,\n",
    "    batch_first=True,\n",
    ").to(device)\n",
    "\n",
    "output = transformer2(cur_img_tokens, tmp)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7364, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(0.1657, device='cuda:0', grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_fc = nn.Linear(hidden_dim, 2).to(device)\n",
    "norm_xy = xy_fc(output).sigmoid()\n",
    "norm_xy.max(), norm_xy.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(164.9437, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(37.1275, device='cuda:0', grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = norm_xy * 224\n",
    "xy.max(), xy.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoDETR(nn.Module):\n",
    "    def __init__(self, boundary_num=80, device=\"cuda\"):\n",
    "        super(DinoDETR, self).__init__()\n",
    "        self.dino4 = get_dino4()\n",
    "        self.raw_dino = get_raw_dino()\n",
    "        # freeze raw_dino\n",
    "        for param in self.raw_dino.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.hidden_dim = 384\n",
    "        self.layernorm = nn.LayerNorm(self.hidden_dim).to(device)\n",
    "        self.pos_enc = PositionalEncoding(self.hidden_dim).to(device)\n",
    "        self.boundary_num = boundary_num\n",
    "        self.queries = nn.Parameter(\n",
    "            torch.rand(boundary_num, self.hidden_dim),\n",
    "        ).to(device)\n",
    "        self.transformer1 = nn.Transformer(\n",
    "            d_model=self.hidden_dim,\n",
    "            nhead=1,\n",
    "            num_encoder_layers=1,\n",
    "            num_decoder_layers=1,\n",
    "            batch_first=True,\n",
    "        ).to(device)\n",
    "        self.transformer2 = nn.Transformer(\n",
    "            d_model=self.hidden_dim,\n",
    "            nhead=1,\n",
    "            num_encoder_layers=1,\n",
    "            num_decoder_layers=1,\n",
    "            batch_first=True,\n",
    "        ).to(device)\n",
    "        self.xy_fc = nn.Linear(self.hidden_dim, 2).to(device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        fir_img: torch.Tensor,\n",
    "        fir_mask: torch.Tensor,\n",
    "        pre_img: torch.Tensor,\n",
    "        pre_mask: torch.Tensor,\n",
    "        cur_img: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        fir_con_feats = self.dino4(torch.cat([fir_img, fir_mask], dim=1))\n",
    "        pre_con_feats = self.dino4(torch.cat([pre_img, pre_mask], dim=1))\n",
    "        fir_con_tokens = get_img_tokens(fir_con_feats)\n",
    "        pre_con_tokens = get_img_tokens(pre_con_feats)\n",
    "        mem_img_tokens = torch.cat((fir_con_tokens, pre_con_tokens), dim=1)\n",
    "        mem_img_tokens = self.layernorm(mem_img_tokens)\n",
    "        mem_img_tokens = self.pos_enc(mem_img_tokens)\n",
    "        B, S, D = mem_img_tokens.shape\n",
    "        queries = self.queries.unsqueeze(0).expand(B, -1, -1)\n",
    "        mem_img_tokens = mem_img_tokens\n",
    "        x = self.transformer1(mem_img_tokens, queries)\n",
    "        cur_img_feats = self.raw_dino(cur_img)\n",
    "        cur_img_tokens = get_img_tokens(cur_img_feats)\n",
    "        cur_img_tokens = self.layernorm(cur_img_tokens)\n",
    "        cur_img_tokens = self.pos_enc(cur_img_tokens)\n",
    "        x = self.transformer2(cur_img_tokens, x)\n",
    "        norm_xy = self.xy_fc(x).sigmoid()\n",
    "        xy = norm_xy * 224\n",
    "        return xy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/facebookresearch_dino_main\n",
      "/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = DinoDETR().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[129.4344, 153.6473],\n",
      "         [ 99.5774, 105.5077],\n",
      "         [ 82.0723, 148.6579],\n",
      "         [ 79.6153, 161.4762],\n",
      "         [ 74.4268, 152.0893],\n",
      "         [111.8938, 123.1397],\n",
      "         [ 87.8885, 155.5473],\n",
      "         [ 98.8817, 161.5239],\n",
      "         [107.7761, 146.3063],\n",
      "         [ 81.8827, 158.4506],\n",
      "         [116.5304, 137.9882],\n",
      "         [ 89.2141, 145.6071],\n",
      "         [ 97.6094, 138.0553],\n",
      "         [ 89.5063, 132.3686],\n",
      "         [ 98.5212, 174.6442],\n",
      "         [ 91.2148, 137.9995],\n",
      "         [114.2734, 152.7850],\n",
      "         [100.6595, 159.2484],\n",
      "         [ 97.6542, 119.3128],\n",
      "         [134.6010, 151.2711],\n",
      "         [ 98.0765, 140.0412],\n",
      "         [118.0413, 162.2771],\n",
      "         [103.3818, 170.9048],\n",
      "         [113.5168, 118.3595],\n",
      "         [ 76.5599, 164.0641],\n",
      "         [ 89.3307, 165.6019],\n",
      "         [ 70.0166, 154.9545],\n",
      "         [ 97.1229, 161.1718],\n",
      "         [114.8269, 155.5305],\n",
      "         [106.6668, 168.3149],\n",
      "         [ 89.1700, 163.9519],\n",
      "         [107.4631, 155.6739],\n",
      "         [103.3237, 168.5255],\n",
      "         [124.3978, 155.1478],\n",
      "         [ 97.1039, 137.5324],\n",
      "         [ 94.1647, 155.2386],\n",
      "         [ 96.6600, 165.3556],\n",
      "         [ 86.2332, 146.4959],\n",
      "         [ 93.0853, 149.0221],\n",
      "         [ 71.2922, 144.2862],\n",
      "         [110.0036, 162.6095],\n",
      "         [102.6932, 171.5809],\n",
      "         [111.7974, 175.5707],\n",
      "         [118.5315, 117.3131],\n",
      "         [ 90.0429, 164.1584],\n",
      "         [122.9113, 143.7581],\n",
      "         [107.3223, 160.1354],\n",
      "         [102.4463, 141.6010],\n",
      "         [109.6047, 132.9082],\n",
      "         [ 89.2245, 166.5451],\n",
      "         [ 79.3354, 144.8262],\n",
      "         [127.0564, 149.5313],\n",
      "         [ 56.0501, 143.0402],\n",
      "         [109.2185, 120.6314],\n",
      "         [ 95.8665, 140.3127],\n",
      "         [ 99.8904, 132.2636],\n",
      "         [109.5373, 157.5687],\n",
      "         [ 89.8695, 162.0984],\n",
      "         [ 91.2326, 127.6621],\n",
      "         [ 60.0629, 103.1613],\n",
      "         [ 99.9053, 124.1129],\n",
      "         [105.1144, 122.5582],\n",
      "         [117.4809, 155.0013],\n",
      "         [ 73.5619, 173.2080],\n",
      "         [110.0376, 129.7867],\n",
      "         [ 88.4921, 160.8977],\n",
      "         [105.5182, 173.6271],\n",
      "         [ 74.9771, 143.9571],\n",
      "         [100.2125, 164.5217],\n",
      "         [108.7885, 141.9691],\n",
      "         [121.8816, 151.3447],\n",
      "         [ 91.5511, 151.7943],\n",
      "         [ 62.3852, 167.3761],\n",
      "         [116.7233, 178.6861],\n",
      "         [ 82.8506, 145.2145],\n",
      "         [ 81.3555, 139.1371],\n",
      "         [126.3946, 139.4243],\n",
      "         [110.9337, 154.0933],\n",
      "         [ 85.0305, 168.2487],\n",
      "         [ 81.2469, 146.6754]],\n",
      "\n",
      "        [[124.3162, 156.7585],\n",
      "         [ 98.2647, 121.6801],\n",
      "         [ 70.3012, 158.4309],\n",
      "         [ 94.7418, 139.0293],\n",
      "         [103.9084, 154.5346],\n",
      "         [ 81.7599, 152.4220],\n",
      "         [ 88.7609, 157.7195],\n",
      "         [ 51.7992, 146.9819],\n",
      "         [ 87.5315, 151.1007],\n",
      "         [106.4757, 169.7086],\n",
      "         [ 96.7467, 127.6772],\n",
      "         [ 75.1998, 163.2014],\n",
      "         [ 80.4865, 117.4965],\n",
      "         [ 86.2370, 148.4477],\n",
      "         [ 80.2315, 159.2909],\n",
      "         [ 60.5304, 184.4906],\n",
      "         [ 94.2429, 178.2233],\n",
      "         [ 90.6983, 180.7029],\n",
      "         [ 83.5283, 145.7588],\n",
      "         [109.5119, 145.6364],\n",
      "         [ 85.4833, 149.2116],\n",
      "         [123.7755, 145.4717],\n",
      "         [117.0904, 169.0094],\n",
      "         [125.6007, 119.9789],\n",
      "         [106.6721, 135.5893],\n",
      "         [ 83.5419, 135.3729],\n",
      "         [ 76.2342, 161.1306],\n",
      "         [ 95.6455, 133.5472],\n",
      "         [ 96.8777, 151.9899],\n",
      "         [108.9977, 165.7737],\n",
      "         [101.1702, 165.0646],\n",
      "         [131.0921, 129.2156],\n",
      "         [ 95.4303, 156.8201],\n",
      "         [104.2999, 144.6284],\n",
      "         [124.0793, 133.8553],\n",
      "         [126.4506, 128.5151],\n",
      "         [ 97.3373, 159.2837],\n",
      "         [ 88.6808, 142.3277],\n",
      "         [ 69.7021, 153.7402],\n",
      "         [ 71.4984, 166.9393],\n",
      "         [120.1223, 124.1502],\n",
      "         [ 79.9709, 159.6697],\n",
      "         [ 99.7000, 165.7451],\n",
      "         [ 99.0658, 146.8493],\n",
      "         [ 67.3418, 128.7875],\n",
      "         [107.8495, 139.1644],\n",
      "         [ 86.5695, 165.0323],\n",
      "         [ 98.6203, 149.2329],\n",
      "         [ 90.5445, 144.1985],\n",
      "         [ 74.2331, 138.7136],\n",
      "         [ 79.9701, 146.4562],\n",
      "         [115.8022, 158.7609],\n",
      "         [ 50.5703, 153.9857],\n",
      "         [ 91.2509, 116.1883],\n",
      "         [141.6497, 160.7612],\n",
      "         [127.8968, 150.3271],\n",
      "         [138.5959, 149.1024],\n",
      "         [122.3030, 170.2587],\n",
      "         [ 93.2322, 139.7870],\n",
      "         [ 92.4484, 140.3801],\n",
      "         [ 88.7490, 131.1391],\n",
      "         [109.7155, 148.7807],\n",
      "         [124.9984, 158.6202],\n",
      "         [ 90.0903, 157.3495],\n",
      "         [128.9011, 129.5019],\n",
      "         [ 78.0027, 124.1004],\n",
      "         [110.3509, 155.9488],\n",
      "         [ 94.0764, 162.0991],\n",
      "         [111.6908, 172.3704],\n",
      "         [ 75.8871, 147.5406],\n",
      "         [111.5577, 162.3886],\n",
      "         [124.6100, 154.4461],\n",
      "         [ 67.3693, 161.5230],\n",
      "         [ 97.9897, 143.9240],\n",
      "         [119.3021, 161.3654],\n",
      "         [116.3300, 140.1852],\n",
      "         [157.5063, 147.0065],\n",
      "         [114.9470, 132.2685],\n",
      "         [ 98.4727, 169.0807],\n",
      "         [ 62.8729, 167.7394]],\n",
      "\n",
      "        [[112.2674, 137.6514],\n",
      "         [ 74.3404, 125.6784],\n",
      "         [ 94.3482, 171.8521],\n",
      "         [ 79.0190, 167.5077],\n",
      "         [ 81.3718, 151.0241],\n",
      "         [ 96.1299, 142.9861],\n",
      "         [100.2420, 162.2676],\n",
      "         [ 49.6869, 156.3530],\n",
      "         [ 92.3381, 166.0663],\n",
      "         [105.6074, 169.1625],\n",
      "         [111.2525, 159.1492],\n",
      "         [109.3478, 134.8691],\n",
      "         [ 82.8053, 160.1878],\n",
      "         [103.6580, 147.0017],\n",
      "         [ 92.7247, 170.9052],\n",
      "         [ 70.3468, 184.4337],\n",
      "         [105.5225, 149.3889],\n",
      "         [125.7103, 144.5875],\n",
      "         [ 65.7361, 150.9665],\n",
      "         [100.4091, 171.8496],\n",
      "         [ 91.7423, 147.9829],\n",
      "         [129.3820, 145.7021],\n",
      "         [126.7695, 156.8586],\n",
      "         [114.7104, 153.2726],\n",
      "         [ 84.6272, 162.6431],\n",
      "         [ 84.9467, 144.4103],\n",
      "         [ 88.6390, 161.3542],\n",
      "         [120.3788, 153.6638],\n",
      "         [ 93.0527, 142.3193],\n",
      "         [109.4271, 154.5039],\n",
      "         [105.7108, 165.7903],\n",
      "         [105.2879, 148.1564],\n",
      "         [106.1311, 159.2063],\n",
      "         [116.1774, 137.6580],\n",
      "         [ 69.3291, 130.3594],\n",
      "         [ 94.4343, 152.1644],\n",
      "         [109.7783, 158.9530],\n",
      "         [ 93.6024, 134.4675],\n",
      "         [ 73.6631, 167.1429],\n",
      "         [ 68.7850, 147.4497],\n",
      "         [ 97.9087, 149.3154],\n",
      "         [ 77.8669, 174.2794],\n",
      "         [ 83.4102, 163.2087],\n",
      "         [ 89.4066, 137.0897],\n",
      "         [ 70.6952, 156.4081],\n",
      "         [113.2839, 146.8831],\n",
      "         [109.4567, 167.1591],\n",
      "         [112.9870, 138.9509],\n",
      "         [ 85.2069, 116.4821],\n",
      "         [ 82.8195, 157.8726],\n",
      "         [ 95.9576, 124.7885],\n",
      "         [116.1376, 163.4350],\n",
      "         [ 59.8030, 150.8669],\n",
      "         [113.5827, 146.8819],\n",
      "         [123.4132, 157.4977],\n",
      "         [ 99.4855, 139.1381],\n",
      "         [ 95.0553, 173.0045],\n",
      "         [106.9994, 153.4725],\n",
      "         [110.4715, 139.0191],\n",
      "         [ 87.1252, 129.8011],\n",
      "         [ 76.1775, 146.8402],\n",
      "         [125.9898, 163.2162],\n",
      "         [112.1563, 159.5814],\n",
      "         [134.1118, 157.9806],\n",
      "         [125.7452, 151.2550],\n",
      "         [114.1436, 155.1081],\n",
      "         [ 79.8898, 162.7958],\n",
      "         [114.4897, 136.2389],\n",
      "         [125.5522, 162.6802],\n",
      "         [ 71.3138, 135.1480],\n",
      "         [ 99.0937, 154.6013],\n",
      "         [122.0393, 151.7870],\n",
      "         [ 84.6144, 148.2376],\n",
      "         [117.4055, 159.2252],\n",
      "         [ 82.1147, 133.9174],\n",
      "         [ 99.5764, 106.2585],\n",
      "         [129.0177, 137.3401],\n",
      "         [110.8677, 147.2413],\n",
      "         [102.8917, 145.8882],\n",
      "         [ 69.0604, 152.4820]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Time: 0.052976131439208984\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(model(fir_img, fir_mask, pre_img, pre_mask, cur_img))\n",
    "end_time = time.time()\n",
    "print(f\"Time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
