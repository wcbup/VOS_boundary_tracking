{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from model import PositionalEncoding, get_bou_features, find_best_shift\n",
    "from model import IterWholeFirst\n",
    "from einops import rearrange\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from featup.util import norm, unnorm\n",
    "from featup.plotting import plot_feats\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary_num = 80\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "first_img = torch.rand(3, 3, 224, 224).to(device)\n",
    "previous_img = torch.rand(3, 3, 224, 224).to(device)\n",
    "current_img = torch.rand(3, 3, 224, 224).to(device)\n",
    "pre_boundary = torch.randint(0, 224, (3, boundary_num, 2)).to(device)\n",
    "first_boundary = torch.randint(0, 224, (3, boundary_num, 2)).to(device)\n",
    "pre_boundary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "featup_backbone = torch.hub.load(\n",
    "    \"mhamilton723/FeatUp\",\n",
    "    \"dino16\",\n",
    "    use_norm=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embed_proj = featup_backbone.model[0].model.patch_embed.proj\n",
    "patch_embed_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_mask = torch.rand(3, 1, 224, 224).to(device)\n",
    "first_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_con = torch.cat([first_img, first_mask], dim=1)\n",
    "first_con.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384, 14, 14])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embed_proj(first_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_sequential(target, source_state, extra_chan=1):\n",
    "\n",
    "    new_dict = OrderedDict()\n",
    "\n",
    "    for k1, v1 in target.state_dict().items():\n",
    "        if not \"num_batches_tracked\" in k1:\n",
    "            if k1 in source_state:\n",
    "                tar_v = source_state[k1]\n",
    "\n",
    "                if v1.shape != tar_v.shape:\n",
    "                    # Init the new segmentation channel with zeros\n",
    "                    # print(v1.shape, tar_v.shape)\n",
    "                    c, _, w, h = v1.shape\n",
    "                    pads = torch.zeros((c, extra_chan, w, h), device=tar_v.device)\n",
    "                    nn.init.orthogonal_(pads)\n",
    "                    tar_v = torch.cat([tar_v, pads], 1)\n",
    "\n",
    "                new_dict[k1] = tar_v\n",
    "\n",
    "    target.load_state_dict(new_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384, 14, 14])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_patch_embed_proj = nn.Conv2d(4, 384, 16, 16).to(device)\n",
    "load_weights_sequential(new_patch_embed_proj, patch_embed_proj.state_dict())\n",
    "new_patch_embed_proj(first_con).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025824022/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384, 224, 224])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featup_backbone(first_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "featup_backbone.model[0].model.patch_embed.proj = new_patch_embed_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 1, 1], expected input[3, 4, 28, 28] to have 3 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m featup_backbone(first_con)\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/mhamilton723_FeatUp_main/hubconf.py:23\u001b[0m, in \u001b[0;36mUpsampledBackbone.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(image), image)\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/VOS_boundary_tracking/FeatUp/featup/upsamplers.py:272\u001b[0m, in \u001b[0;36mJBUStack.forward\u001b[0;34m(self, source, guidance)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, source, guidance):\n\u001b[0;32m--> 272\u001b[0m     source_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(source, guidance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1)\n\u001b[1;32m    273\u001b[0m     source_4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(source_2, guidance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2)\n\u001b[1;32m    274\u001b[0m     source_8 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(source_4, guidance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3)\n",
      "File \u001b[0;32m~/VOS_boundary_tracking/FeatUp/featup/upsamplers.py:268\u001b[0m, in \u001b[0;36mJBUStack.upsample\u001b[0;34m(self, source, guidance, up)\u001b[0m\n\u001b[1;32m    266\u001b[0m _, _, h, w \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    267\u001b[0m small_guidance \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(guidance, (h \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, w \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 268\u001b[0m upsampled \u001b[38;5;241m=\u001b[39m up(source, small_guidance)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m upsampled\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/VOS_boundary_tracking/FeatUp/featup/upsamplers.py:236\u001b[0m, in \u001b[0;36mJBULearnedRange.forward\u001b[0;34m(self, source, guidance)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (SB \u001b[38;5;241m==\u001b[39m GB)\n\u001b[1;32m    235\u001b[0m spatial_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_spatial_kernel(source\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 236\u001b[0m range_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_range_kernel(guidance)\n\u001b[1;32m    238\u001b[0m combined_kernel \u001b[38;5;241m=\u001b[39m range_kernel \u001b[38;5;241m*\u001b[39m spatial_kernel\n\u001b[1;32m    239\u001b[0m combined_kernel \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m combined_kernel\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m1e-7\u001b[39m)\n",
      "File \u001b[0;32m~/VOS_boundary_tracking/FeatUp/featup/upsamplers.py:215\u001b[0m, in \u001b[0;36mJBULearnedRange.get_range_kernel\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_range_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    214\u001b[0m     GB, GC, GH, GW \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 215\u001b[0m     proj_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrange_proj(x)\n\u001b[1;32m    216\u001b[0m     proj_x_padded \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(proj_x, pad\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mradius] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    217\u001b[0m     queries \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mUnfold(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter)(proj_x_padded) \\\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;241m.\u001b[39mreshape((GB, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter, GH, GW)) \\\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/work3/s232248/miniconda3/envs/ras/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 1, 1], expected input[3, 4, 28, 28] to have 3 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "featup_backbone(first_con).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featup_backbone.model[0].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232248/miniconda3/envs/vos/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025824022/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 384, 224, 224]),\n",
       " torch.Size([3, 384, 224, 224]),\n",
       " torch.Size([3, 384, 224, 224]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_img_feats = featup_backbone(previous_img)\n",
    "cur_img_feats = featup_backbone(current_img)\n",
    "fir_img_feats = featup_backbone(first_img)\n",
    "pre_img_feats.shape, cur_img_feats.shape, fir_img_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_features(\n",
    "    img_features: torch.Tensor,\n",
    "    boundary: torch.Tensor,\n",
    "    scale_level: int,\n",
    "):\n",
    "    device = img_features.device\n",
    "    if scale_level > 0:\n",
    "        img_features = F.avg_pool2d(\n",
    "            img_features,\n",
    "            2**scale_level,\n",
    "            2**scale_level,\n",
    "        )\n",
    "        boundary = boundary // (2**scale_level)\n",
    "    img_features = F.pad(img_features, (1, 1, 1, 1), \"constant\", 0)\n",
    "    neighor_features = []\n",
    "    for x_offset in range(-1, 2):\n",
    "        for y_offset in range(-1, 2):\n",
    "            neighor_features.append(\n",
    "                get_bou_features(\n",
    "                    img_features,\n",
    "                    boundary + torch.tensor([x_offset, y_offset]).to(device),\n",
    "                )\n",
    "            )\n",
    "    return torch.cat(neighor_features, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_feats = []\n",
    "for scale_level in range(4):\n",
    "    neigh_feats.append(\n",
    "        get_neighbor_features(\n",
    "            cur_img_feats,\n",
    "            pre_boundary,\n",
    "            scale_level,\n",
    "        ).permute(0, 2, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 720, 384])\n",
      "torch.Size([3, 720, 384])\n",
      "torch.Size([3, 720, 384])\n",
      "torch.Size([3, 720, 384])\n"
     ]
    }
   ],
   "source": [
    "for neigh_feat in neigh_feats:\n",
    "    print(neigh_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_token_fc = nn.Linear(384, 384 + 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n"
     ]
    }
   ],
   "source": [
    "neigh_tokens = []\n",
    "for scale_level in range(4):\n",
    "    neigh_tokens.append(neigh_token_fc(neigh_feats[scale_level]))\n",
    "    print(neigh_tokens[scale_level].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(384 + 2).to(device)\n",
    "position_enc = PositionalEncoding(384 + 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n"
     ]
    }
   ],
   "source": [
    "for scale_level in range(4):\n",
    "    neigh_tokens[scale_level] = position_enc(\n",
    "        layer_norm(\n",
    "            neigh_tokens[scale_level],\n",
    "        )\n",
    "    )\n",
    "    print(neigh_tokens[scale_level].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232248/miniconda3/envs/vos/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=384 + 2,\n",
    "    nhead=1,\n",
    "    batch_first=True,\n",
    ")\n",
    "transformer_encoder = nn.TransformerEncoder(\n",
    "    encoder_layer,\n",
    "    num_layers=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n",
      "torch.Size([3, 720, 386])\n"
     ]
    }
   ],
   "source": [
    "memorys = []\n",
    "for scale_level in range(4):\n",
    "    memorys.append(\n",
    "        transformer_encoder(\n",
    "            neigh_tokens[scale_level],\n",
    "        )\n",
    "    )\n",
    "    print(memorys[scale_level].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 384, 80]), torch.Size([3, 384, 80]), torch.Size([3, 384, 80]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_bou_feats = get_bou_features(pre_img_feats, pre_boundary)\n",
    "cur_bou_feats = get_bou_features(cur_img_feats, pre_boundary)\n",
    "fir_bou_feats = get_bou_features(fir_img_feats, first_boundary)\n",
    "pre_bou_feats.shape, cur_bou_feats.shape, fir_bou_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 80, 384]), torch.Size([3, 80, 384]), torch.Size([3, 80, 384]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fir_bou_feats = fir_bou_feats.permute(0, 2, 1)\n",
    "pre_bou_feats = pre_bou_feats.permute(0, 2, 1)\n",
    "cur_bou_feats = cur_bou_feats.permute(0, 2, 1)\n",
    "fir_bou_feats.shape, pre_bou_feats.shape, cur_bou_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 80, 386]), torch.Size([3, 80, 386]), torch.Size([3, 80, 386]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fir_tokens = torch.cat([fir_bou_feats, first_boundary.float()], dim=2)\n",
    "pre_tokens = torch.cat([pre_bou_feats, pre_boundary.float()], dim=2)\n",
    "cur_tokens = torch.cat([cur_bou_feats, pre_boundary.float()], dim=2)\n",
    "fir_tokens.shape, pre_tokens.shape, cur_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(\n",
    "    d_model=384 + 2,\n",
    "    nhead=1,\n",
    "    batch_first=True,\n",
    ")\n",
    "transformer_decoder = nn.TransformerDecoder(\n",
    "    decoder_layer,\n",
    "    num_layers=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 240, 386])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = torch.cat([pre_tokens, cur_tokens, fir_tokens], dim=1)\n",
    "input_tokens = layer_norm(input_tokens)\n",
    "input_tokens = position_enc(input_tokens)\n",
    "input_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 240, 386])\n",
      "torch.Size([3, 240, 386])\n",
      "torch.Size([3, 240, 386])\n",
      "torch.Size([3, 240, 386])\n"
     ]
    }
   ],
   "source": [
    "for scale_level in reversed(range(4)):\n",
    "    input_tokens = transformer_decoder(\n",
    "        input_tokens,\n",
    "        memorys[scale_level],\n",
    "    )\n",
    "    print(input_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80, 386])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tokens = input_tokens[\n",
    "    :,\n",
    "    boundary_num : 2 * boundary_num,\n",
    "    :,\n",
    "]\n",
    "out_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_offset_fc = nn.Linear(384 + 2, 2).to(device)\n",
    "xy_offset = xy_offset_fc(out_tokens)\n",
    "xy_offset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatupNei(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatupNei, self).__init__()\n",
    "        self.bou_num = 80\n",
    "        d_token = 384 + 2\n",
    "        self.scale_num = 4\n",
    "\n",
    "        self.backbone = torch.hub.load(\n",
    "            \"mhamilton723/FeatUp\",\n",
    "            \"dino16\",\n",
    "            use_norm=True,\n",
    "        ).to(device)\n",
    "        # freeze the backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.neighor_fc = nn.Linear(384, d_token).to(device)\n",
    "        self.layer_norm = nn.LayerNorm(d_token).to(device)\n",
    "        self.position_enc = PositionalEncoding(d_token).to(device)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=1,\n",
    "        ).to(device)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=1,\n",
    "        ).to(device)\n",
    "        self.xy_offset_fc = nn.Linear(d_token, 2).to(device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        fir_img: torch.Tensor,\n",
    "        fir_bou: torch.Tensor,\n",
    "        pre_img: torch.Tensor,\n",
    "        cur_img: torch.Tensor,\n",
    "        pre_bou: torch.Tensor,\n",
    "    ):\n",
    "        best_shift = find_best_shift(pre_bou, fir_bou)\n",
    "        for i in range(len(best_shift)):\n",
    "            fir_bou[i] = fir_bou[i].roll(best_shift[i], 0)\n",
    "\n",
    "        fir_img_feats = self.backbone(fir_img)\n",
    "        pre_img_feats = self.backbone(pre_img)\n",
    "        cur_img_feats = self.backbone(cur_img)\n",
    "\n",
    "        neigh_feats = []\n",
    "        for scale_level in range(self.scale_num):\n",
    "            neigh_feats.append(\n",
    "                get_neighbor_features(\n",
    "                    cur_img_feats,\n",
    "                    pre_bou,\n",
    "                    scale_level,\n",
    "                ).permute(0, 2, 1)\n",
    "            )\n",
    "        neigh_tokens = []\n",
    "        for scale_level in range(self.scale_num):\n",
    "            neigh_tokens.append(\n",
    "                self.neighor_fc(\n",
    "                    neigh_feats[scale_level],\n",
    "                ),\n",
    "            )\n",
    "        for scale_level in range(self.scale_num):\n",
    "            neigh_tokens[scale_level] = self.position_enc(\n",
    "                self.layer_norm(\n",
    "                    neigh_tokens[scale_level],\n",
    "                ),\n",
    "            )\n",
    "        memorys = []\n",
    "        for scale_level in range(self.scale_num):\n",
    "            memorys.append(\n",
    "                self.transformer_encoder(\n",
    "                    neigh_tokens[scale_level],\n",
    "                ),\n",
    "            )\n",
    "        \n",
    "        pre_bou_feats = get_bou_features(pre_img_feats, pre_bou)\n",
    "        cur_bou_feats = get_bou_features(cur_img_feats, pre_bou)\n",
    "        fir_bou_feats = get_bou_features(fir_img_feats, fir_bou)\n",
    "        fir_bou_feats = fir_bou_feats.permute(0, 2, 1)\n",
    "        pre_bou_feats = pre_bou_feats.permute(0, 2, 1)\n",
    "        cur_bou_feats = cur_bou_feats.permute(0, 2, 1)\n",
    "        fir_tokens = torch.cat([fir_bou_feats, fir_bou.float()], dim=2)\n",
    "        pre_tokens = torch.cat([pre_bou_feats, pre_bou.float()], dim=2)\n",
    "        cur_tokens = torch.cat([cur_bou_feats, pre_bou.float()], dim=2)\n",
    "        input_tokens = torch.cat([pre_tokens, cur_tokens, fir_tokens], dim=1)\n",
    "        input_tokens = self.layer_norm(input_tokens)\n",
    "        input_tokens = self.position_enc(input_tokens)\n",
    "        for scale_level in reversed(range(self.scale_num)):\n",
    "            input_tokens = self.transformer_decoder(\n",
    "                input_tokens,\n",
    "                memorys[scale_level],\n",
    "            )\n",
    "        out_tokens = input_tokens[:, self.bou_num : 2 * self.bou_num, :]\n",
    "        xy_offset = self.xy_offset_fc(out_tokens)\n",
    "        result = xy_offset + pre_bou.float()\n",
    "        return [result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /zhome/32/f/202284/.cache/torch/hub/facebookresearch_dino_main\n",
      "/work3/s232248/miniconda3/envs/vos/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = FeatupNei().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[153.1673, 195.6584],\n",
       "         [ 27.4937,  31.4670],\n",
       "         [150.3264,  70.5704],\n",
       "         [156.4951, 104.6779],\n",
       "         [213.1922, 155.4496],\n",
       "         [132.9963,  62.5916],\n",
       "         [214.2008, 121.6199],\n",
       "         [ 68.6694, 137.4595],\n",
       "         [205.1222,  80.6518],\n",
       "         [136.2520,  90.4591],\n",
       "         [216.7162,  96.6581],\n",
       "         [ 19.2738,  97.3381],\n",
       "         [ 38.0802, 188.5018],\n",
       "         [ 79.4439,  45.3750],\n",
       "         [ 92.1500, 184.3403],\n",
       "         [204.6061,  29.6343],\n",
       "         [ 31.1597,  50.6149],\n",
       "         [ 50.5443, 113.3948],\n",
       "         [ 42.0186, 116.3720],\n",
       "         [ 73.3154, 119.4523],\n",
       "         [110.8636, 196.4116],\n",
       "         [ 94.1512, 153.4003],\n",
       "         [120.6401,  19.5699],\n",
       "         [ 73.4373, 106.4883],\n",
       "         [160.1704, 212.2492],\n",
       "         [ 48.2539, 169.3328],\n",
       "         [146.7191, 179.5117],\n",
       "         [  9.8852, 199.5704],\n",
       "         [ 40.9352, 210.2940],\n",
       "         [ 16.4163,  60.3045],\n",
       "         [ 37.9838, 184.9747],\n",
       "         [ 43.0824,  54.5503],\n",
       "         [ 35.6231, 199.4666],\n",
       "         [ 70.8092, 177.6678],\n",
       "         [100.9419, 135.4158],\n",
       "         [  4.7662,  18.4623],\n",
       "         [ 89.0650,  69.6069],\n",
       "         [ 54.8213,  89.6938],\n",
       "         [ 44.7575,  63.6329],\n",
       "         [130.9733, 197.7610],\n",
       "         [ 72.9165,  15.5167],\n",
       "         [212.0622, 187.6225],\n",
       "         [187.3353, 117.3655],\n",
       "         [129.2149,  18.6028],\n",
       "         [ 22.7408, 166.4136],\n",
       "         [ 46.7747, 125.3398],\n",
       "         [  8.6980, 189.5179],\n",
       "         [126.7733,  77.5188],\n",
       "         [219.3716,  76.8741],\n",
       "         [207.4168, 141.8401],\n",
       "         [ 15.2923,  82.6482],\n",
       "         [163.6116, 110.3029],\n",
       "         [190.2868, 108.4298],\n",
       "         [ 80.0161, 139.4834],\n",
       "         [104.4273,  19.7362],\n",
       "         [172.9532, 117.6037],\n",
       "         [160.5084,  59.6544],\n",
       "         [146.2077, 204.6387],\n",
       "         [222.3577, 187.4409],\n",
       "         [ 20.8214, 180.4924],\n",
       "         [174.4504,  37.5360],\n",
       "         [ 64.1187,  37.7600],\n",
       "         [135.4339,  53.4773],\n",
       "         [ 91.9605, 172.4395],\n",
       "         [191.1770, 176.8148],\n",
       "         [ 18.5667, 122.6735],\n",
       "         [ 57.9476, 194.9347],\n",
       "         [ 68.0002,  99.5219],\n",
       "         [128.8855, 219.5472],\n",
       "         [ 12.3076,  49.5044],\n",
       "         [205.7297,   1.7347],\n",
       "         [129.2081, 205.5651],\n",
       "         [ 87.8970, 129.4132],\n",
       "         [ 68.9649, 154.2221],\n",
       "         [104.0260, 102.4138],\n",
       "         [113.4359, 171.7427],\n",
       "         [195.4278,  29.6229],\n",
       "         [ 28.8535, 199.4576],\n",
       "         [  9.5361, 106.5672],\n",
       "         [223.0881, 146.7021]],\n",
       "\n",
       "        [[128.9385, 204.6371],\n",
       "         [202.1293, 203.5656],\n",
       "         [ 35.2243,  16.8872],\n",
       "         [163.3329,  50.5723],\n",
       "         [ 97.2320,  53.6262],\n",
       "         [ 61.8815, 219.2426],\n",
       "         [105.8197, 205.3265],\n",
       "         [ 63.6485,  73.2091],\n",
       "         [102.2923, 110.6046],\n",
       "         [ 69.8421, 139.7074],\n",
       "         [ 74.7379, 145.5676],\n",
       "         [ 42.3499,  33.6407],\n",
       "         [ 14.0277,  66.2520],\n",
       "         [130.6822,  56.5683],\n",
       "         [ 75.5840,  16.3252],\n",
       "         [169.5418,  98.2418],\n",
       "         [ 98.4792, 170.0342],\n",
       "         [214.7260,  24.3601],\n",
       "         [ 99.7238,  19.6758],\n",
       "         [118.4240,  45.4727],\n",
       "         [167.2965, 218.6114],\n",
       "         [ 70.2774,  40.3469],\n",
       "         [200.7281,   8.6167],\n",
       "         [ 45.0076,  24.5413],\n",
       "         [ 17.3379,  39.0378],\n",
       "         [158.7181,  80.0572],\n",
       "         [181.6495,  69.4842],\n",
       "         [207.7999, 155.2659],\n",
       "         [196.3349,  33.5182],\n",
       "         [151.8670,  22.4411],\n",
       "         [119.9384, 192.7446],\n",
       "         [137.4580, 123.4230],\n",
       "         [ 45.1066,  82.6775],\n",
       "         [194.6241,  64.3383],\n",
       "         [123.6002,  79.2245],\n",
       "         [138.1291, 122.4378],\n",
       "         [165.0122,  34.2495],\n",
       "         [196.6249,  79.6619],\n",
       "         [ 77.6802,  91.5815],\n",
       "         [162.4073,  31.3136],\n",
       "         [153.0469,  50.7076],\n",
       "         [189.1551, 216.5694],\n",
       "         [ 66.8754, 212.4183],\n",
       "         [130.0682, 168.6429],\n",
       "         [ 48.0367,  50.6559],\n",
       "         [218.1206, 135.7125],\n",
       "         [116.9372, 136.4194],\n",
       "         [ 70.2072,  83.5721],\n",
       "         [223.3906,  93.7874],\n",
       "         [134.0784, 208.3849],\n",
       "         [ 53.9178, 202.3514],\n",
       "         [188.2779, 169.1709],\n",
       "         [133.5393,  55.2910],\n",
       "         [174.1414, 211.4263],\n",
       "         [ 79.1629, 195.5515],\n",
       "         [124.2893,  95.6044],\n",
       "         [ 47.0608,  47.6477],\n",
       "         [136.4502, 149.2759],\n",
       "         [119.1861,  98.6062],\n",
       "         [ 36.1624, 128.6358],\n",
       "         [ 63.8075, 211.6836],\n",
       "         [ 28.7640, 136.5355],\n",
       "         [109.3018,  44.6607],\n",
       "         [  3.8528, 160.4665],\n",
       "         [153.0466, 195.5951],\n",
       "         [205.1741, 176.5165],\n",
       "         [140.0479, 144.8548],\n",
       "         [ 60.7265,  98.9751],\n",
       "         [219.4689,  28.5603],\n",
       "         [179.2858, 201.8850],\n",
       "         [171.3812,   1.6249],\n",
       "         [146.5540,  57.5629],\n",
       "         [121.3168, 103.6897],\n",
       "         [129.2218, 165.4577],\n",
       "         [ 63.2126,  70.5649],\n",
       "         [ 35.1609, 121.5051],\n",
       "         [135.4664,  81.5362],\n",
       "         [206.4218,  90.5242],\n",
       "         [188.6430,  44.9333],\n",
       "         [  6.1012, 182.4483]],\n",
       "\n",
       "        [[138.1532, 109.4512],\n",
       "         [  1.9396, 152.3152],\n",
       "         [151.2531,  21.8851],\n",
       "         [ 82.9931, 217.5101],\n",
       "         [152.3232,  49.7706],\n",
       "         [119.9637, 138.5703],\n",
       "         [ 86.3175,  27.6876],\n",
       "         [ 59.7276, 116.3824],\n",
       "         [179.1140, 130.9048],\n",
       "         [ 21.7793,  59.5827],\n",
       "         [127.6516,  18.5819],\n",
       "         [ 90.8487,   0.8267],\n",
       "         [168.7040,  11.5305],\n",
       "         [ 48.1684, 149.3819],\n",
       "         [194.2953, 198.4010],\n",
       "         [113.7923, 142.4651],\n",
       "         [107.1178, 180.3479],\n",
       "         [  1.0062, 142.4437],\n",
       "         [196.2706, 155.6074],\n",
       "         [164.0631,  85.6753],\n",
       "         [168.3200, 181.5325],\n",
       "         [195.5070,  47.6783],\n",
       "         [ 81.3071,  17.6618],\n",
       "         [220.1491, 164.4335],\n",
       "         [116.2451,  95.4989],\n",
       "         [ 78.6055,  61.3011],\n",
       "         [ 75.9110, 124.7264],\n",
       "         [124.1276, 198.5714],\n",
       "         [207.7634,  16.6478],\n",
       "         [ 94.8714, 159.5948],\n",
       "         [ 58.0865, 217.3528],\n",
       "         [ 77.1242, 117.3793],\n",
       "         [196.1696, 216.5527],\n",
       "         [113.5569,  40.3693],\n",
       "         [173.1619,  85.4870],\n",
       "         [ 66.8570, 149.4330],\n",
       "         [ 36.9358, 127.3647],\n",
       "         [100.0115,  26.3228],\n",
       "         [218.8540, 191.8136],\n",
       "         [ 77.8704, 173.5248],\n",
       "         [117.4050, 113.4783],\n",
       "         [117.0045,  67.5457],\n",
       "         [162.4710,  62.4016],\n",
       "         [121.0188, 166.3827],\n",
       "         [202.5671,  25.7151],\n",
       "         [161.8614, 185.5648],\n",
       "         [  3.7147,  55.6284],\n",
       "         [  4.6833, 123.5856],\n",
       "         [182.3665,   6.8932],\n",
       "         [ 54.8316, 169.6406],\n",
       "         [ 27.0480,  77.4741],\n",
       "         [ 66.7805,  28.5003],\n",
       "         [ 81.1656,  57.4924],\n",
       "         [155.4518, 146.4611],\n",
       "         [ 90.7515, 166.5349],\n",
       "         [141.1910, 104.8299],\n",
       "         [ 72.2304,  18.7573],\n",
       "         [138.2653, 218.7128],\n",
       "         [ 25.9932,  85.5029],\n",
       "         [ 49.3225,  18.7105],\n",
       "         [ 74.0979,  79.6663],\n",
       "         [ 11.8330, 126.4453],\n",
       "         [ 56.0454, 170.6333],\n",
       "         [182.9941, 212.7569],\n",
       "         [176.8952, 196.7991],\n",
       "         [209.6606,  53.8929],\n",
       "         [ 16.1865,  17.9650],\n",
       "         [  6.7734,  18.8280],\n",
       "         [102.1998, 198.4915],\n",
       "         [130.7864, 107.4059],\n",
       "         [ 93.2712,  51.7726],\n",
       "         [ 44.0492,  94.5714],\n",
       "         [ 13.1224, 155.5466],\n",
       "         [ 38.0010, 160.3633],\n",
       "         [ 85.5432,   0.6125],\n",
       "         [ 10.6789, 141.6272],\n",
       "         [157.0924, 156.6446],\n",
       "         [206.3991,  14.7086],\n",
       "         [  4.7494, 208.4930],\n",
       "         [  0.7903, 205.5134]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    first_img,\n",
    "    first_boundary,\n",
    "    previous_img,\n",
    "    current_img,\n",
    "    pre_boundary,\n",
    ")[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
